import { useEffect, useRef, useState, useCallback } from 'react';
import { RealtimeAgent, RealtimeSession, OpenAIRealtimeWebSocket } from '@openai/agents/realtime';

interface RealtimeAgentConfig {
  customerLanguage?: string;
}

interface ChatMessage {
  id: string;
  role: 'user' | 'assistant';
  content: string;
  timestamp: number;
  type: 'text' | 'audio';
}

export function useRealtimeAgent({ customerLanguage = 'English' }: RealtimeAgentConfig) {
  const [isConnected, setIsConnected] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [messages, setMessages] = useState<ChatMessage[]>([]);
  const [isUserSpeaking, setIsUserSpeaking] = useState(false);
  const [currentUserTranscript, setCurrentUserTranscript] = useState('');
  const [currentAssistantTranscript, setCurrentAssistantTranscript] = useState('');
  const [speechRecognition, setSpeechRecognition] = useState<SpeechRecognition | null>(null);
  const [error, setError] = useState<string | null>(null);
  const [isConnecting, setIsConnecting] = useState(false);

  const sessionRef = useRef<RealtimeSession | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const workletNodeRef = useRef<AudioWorkletNode | null>(null);
  const playbackAudioContextRef = useRef<AudioContext | null>(null);
  const audioChunksRef = useRef<ArrayBuffer[]>([]);
  const isPlayingAudioRef = useRef<boolean>(false);
  const audioSourceRef = useRef<AudioBufferSourceNode | null>(null);
  const nextStartTimeRef = useRef<number>(0);
  const scheduledSourcesRef = useRef<AudioBufferSourceNode[]>([]);

  // ë²ˆì—­ ì—ì´ì „íŠ¸ ìƒì„±
  const createAgent = useCallback(() => {
    const isEnglish = customerLanguage === 'English';
    const isKoreanInput = true; // ê¸°ë³¸ì ìœ¼ë¡œ í•œêµ­ì–´ ì…ë ¥ìœ¼ë¡œ ê°€ì •

    // customerLanguageì— ë”°ë¥¸ ì „ë¬¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
    const getTranslationInstructions = () => {
      // ì–¸ì–´ë³„ ìš©ì–´ì§‘ ë§¤í•‘
      const getSajuTerms = (targetLang: string) => {
        if (targetLang === 'English') {
          return `SAJU TERMINOLOGY (Korean â†” English):
ì‚¬ì£¼ = Four Pillars, ëª…ë¦¬ = destiny analysis, ìš´ì„¸ = fortune
ì¬ìš´ = financial luck, ì—°ì• ìš´ = love fortune, ê±´ê°•ìš´ = health fortune
ê¶í•© = compatibility, ì˜¤í–‰ = Five Elements, ì²œê°„ = Heavenly Stems
ì§€ì§€ = Earthly Branches, ëŒ€ìš´ = major fortune period, ë…„ìš´ = yearly fortune
ì›”ìš´ = monthly fortune, ì¼ìš´ = daily fortune, ì‹ ì‚´ = spiritual influences`;
        } else if (targetLang === 'Chinese') {
          return `SAJU TERMINOLOGY (Korean â†” Chinese):
ì‚¬ì£¼ = å››æŸ±, ëª…ë¦¬ = å‘½ç†, ìš´ì„¸ = é‹å‹¢
ì¬ìš´ = è²¡é‹, ì—°ì• ìš´ = æˆ€æ„›é‹, ê±´ê°•ìš´ = å¥åº·é‹
ê¶í•© = åˆå©š, ì˜¤í–‰ = äº”è¡Œ, ì²œê°„ = å¤©å¹²
ì§€ì§€ = åœ°æ”¯, ëŒ€ìš´ = å¤§é‹, ë…„ìš´ = å¹´é‹`;
        } else {
          return `SAJU BASIC TERMS:
ì‚¬ì£¼ = Four Pillars, ìš´ì„¸ = fortune, ì¬ìš´ = wealth luck
ì—°ì• ìš´ = love fortune, ê±´ê°•ìš´ = health fortune, ê¶í•© = compatibility`;
        }
      };

      // ì˜ˆì‹œ ìƒì„±
      const getExamples = (targetLang: string) => {
        if (targetLang === 'English') {
          return `EXAMPLES:
Input: "ì•ˆë…•í•˜ì„¸ìš”" â†’ Output: "Hello"
Input: "How are you?" â†’ Output: "ì–´ë–»ê²Œ ì§€ë‚´ì„¸ìš”?"
Input: "ì˜¤ëŠ˜ ìš´ì„¸ ì¢€ ë´ì£¼ì„¸ìš”" â†’ Output: "Please look at today's fortune"
Input: "What is my luck today?" â†’ Output: "ì˜¤ëŠ˜ ì œ ìš´ì€ ì–´ë–¤ê°€ìš”?"`;
        } else if (targetLang === 'Chinese') {
          return `EXAMPLES:
Input: "ì•ˆë…•í•˜ì„¸ìš”" â†’ Output: "æ‚¨å¥½"
Input: "ä½ å¥½" â†’ Output: "ì•ˆë…•í•˜ì„¸ìš”"
Input: "ì˜¤ëŠ˜ ìš´ì„¸ ì¢€ ë´ì£¼ì„¸ìš”" â†’ Output: "è«‹çœ‹çœ‹ä»Šå¤©çš„é‹å‹¢"
Input: "ä»Šå¤©é‹æ°£æ€éº¼æ¨£ï¼Ÿ" â†’ Output: "ì˜¤ëŠ˜ ìš´ì„¸ëŠ” ì–´ë–¤ê°€ìš”?"`;
        } else {
          return `EXAMPLES:
Input: "ì•ˆë…•í•˜ì„¸ìš”" â†’ Output: "Hello" (in ${targetLang})
Input: "${targetLang} greeting" â†’ Output: "ì•ˆë…•í•˜ì„¸ìš”"`;
        }
      };

      return `STRICT TRANSLATION MODE ONLY - NO CONVERSATIONS ALLOWED

You are a translation machine specialized in Saju (Four Pillars) fortune-telling. You MUST translate every single input without exception.

MANDATORY RULES - NEVER BREAK THESE:
1. Korean input â†’ ALWAYS translate to ${customerLanguage}
2. ANY non-Korean input (${customerLanguage}, English, Chinese, Japanese, etc.) â†’ ALWAYS translate to Korean ONLY
3. NEVER answer questions, NEVER have conversations
4. NEVER say things like "I can help you", "What would you like", etc.
5. ONLY output the direct translation, nothing else

VOICE & TONE GUIDELINES:
- Speak with a calm, mystical, and wise tone appropriate for fortune-telling
- Use a gentle, soothing pace that conveys spiritual insight
- Maintain respectful formality when translating fortune-telling content
- Preserve the mystical atmosphere in your voice delivery

DETECTION & TRANSLATION RULES:
- If input sounds Korean â†’ translate to ${customerLanguage} immediately
- If input sounds like ANY other language â†’ translate to Korean ONLY
- This includes ${customerLanguage}, English, Chinese, Japanese, and all other languages
- ALL non-Korean languages must become Korean
- NEVER ask for clarification, just translate

${getSajuTerms(customerLanguage)}

TRANSLATION EXAMPLES:
Korean â†’ ${customerLanguage}:
Input: "ì•ˆë…•í•˜ì„¸ìš”" â†’ Output: ${customerLanguage === 'English' ? '"Hello"' : customerLanguage === 'Chinese' ? '"æ‚¨å¥½"' : `"Hello" (in ${customerLanguage})`}
Input: "ì˜¤ëŠ˜ ìš´ì„¸ ì¢€ ë´ì£¼ì„¸ìš”" â†’ Output: ${customerLanguage === 'English' ? '"Please look at today\'s fortune"' : customerLanguage === 'Chinese' ? '"è«‹çœ‹çœ‹ä»Šå¤©çš„é‹å‹¢"' : `"Please look at today's fortune" (in ${customerLanguage})`}

ANY Language â†’ Korean:
Input: "Hello" â†’ Output: "ì•ˆë…•í•˜ì„¸ìš”"
Input: "How are you?" â†’ Output: "ì–´ë–»ê²Œ ì§€ë‚´ì„¸ìš”?"
Input: "What is my luck?" â†’ Output: "ì œ ìš´ì€ ì–´ë–¤ê°€ìš”?"
${customerLanguage === 'Chinese' ? `Input: "ä½ å¥½" â†’ Output: "ì•ˆë…•í•˜ì„¸ìš”"
Input: "ä»Šå¤©é‹æ°£æ€éº¼æ¨£ï¼Ÿ" â†’ Output: "ì˜¤ëŠ˜ ìš´ì„¸ëŠ” ì–´ë–¤ê°€ìš”?"` : ''}

ABSOLUTELY FORBIDDEN:
- Answering questions instead of translating
- Having conversations or giving advice
- Asking "What can I help you with?"
- Explaining what you are or what you do
- ANY response that is not a direct translation

YOU ARE A TRANSLATION MACHINE ONLY. TRANSLATE EVERYTHING.
KOREAN â†’ ${customerLanguage.toUpperCase()} | ALL OTHER LANGUAGES â†’ KOREAN
SPEAK WITH MYSTICAL, WISE TONE FOR SAJU TRANSLATIONS.`;
    };

    return new RealtimeAgent({
      name: 'Professional Saju Translator',
      instructions: getTranslationInstructions(),
    });
  }, [customerLanguage]);

  // ì„¸ì…˜ ì—°ê²°
  const connect = useCallback(async () => {
    if (isConnected || isConnecting) return;

    setIsConnecting(true);
    setError(null);

    try {
      const apiKey = import.meta.env.VITE_OPENAI_API_KEY;
      if (!apiKey) {
        throw new Error('OpenAI API key not found. Please set VITE_OPENAI_API_KEY in your .env file');
      }

      const agent = createAgent();

      // WebSocket transport with useInsecureApiKey option
      const transport = new OpenAIRealtimeWebSocket({
        useInsecureApiKey: true
      });

      const session = new RealtimeSession(agent, {
        model: 'gpt-4o-realtime-preview-2024-10-01',
        transport,
        config: {
          inputAudioFormat: 'pcm16',
          outputAudioFormat: 'pcm16',
          voice: 'coral', // ì‚¬ì£¼í’€ì´ì— ì í•©í•œ ë”°ëœ»í•˜ê³  ì°¨ë¶„í•œ ì—¬ì„± ëª©ì†Œë¦¬
          inputAudioTranscription: {
            model: 'whisper-1'
          },
          outputAudioTranscription: {
            enabled: true
          },
          turnDetection: {
            type: 'server_vad',
            threshold: 0.1, // ë” ë¯¼ê°í•˜ê²Œ ì„¤ì •
            prefixPaddingMs: 200, // ì§§ê²Œ
            silenceDurationMs: 500, // ë” ë¹ ë¥´ê²Œ ì‘ë‹µ
          },
          // ë§í•˜ê¸° ì†ë„ì™€ í†¤ ì¡°ì ˆ (ê°€ëŠ¥í•œ ê²½ìš°)
          maxResponseOutputTokens: 150, // ê°„ê²°í•œ ë²ˆì—­ì„ ìœ„í•´ í† í° ì œí•œ
        },
      });

      // PCM16ì„ WAVë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
      const pcm16ToWav = (pcm16Data: ArrayBuffer, sampleRate: number = 24000) => {
        const buffer = new ArrayBuffer(44 + pcm16Data.byteLength);
        const view = new DataView(buffer);

        // WAV í—¤ë” ì‘ì„±
        const writeString = (offset: number, string: string) => {
          for (let i = 0; i < string.length; i++) {
            view.setUint8(offset + i, string.charCodeAt(i));
          }
        };

        writeString(0, 'RIFF');
        view.setUint32(4, 36 + pcm16Data.byteLength, true);
        writeString(8, 'WAVE');
        writeString(12, 'fmt ');
        view.setUint32(16, 16, true); // Sub-chunk size
        view.setUint16(20, 1, true); // Audio format (PCM)
        view.setUint16(22, 1, true); // Number of channels (mono)
        view.setUint32(24, sampleRate, true); // Sample rate
        view.setUint32(28, sampleRate * 2, true); // Byte rate
        view.setUint16(32, 2, true); // Block align
        view.setUint16(34, 16, true); // Bits per sample
        writeString(36, 'data');
        view.setUint32(40, pcm16Data.byteLength, true);

        // PCM16 ë°ì´í„° ë³µì‚¬
        const pcm16View = new Int16Array(pcm16Data);
        const wavView = new Int16Array(buffer, 44);
        wavView.set(pcm16View);

        return buffer;
      };

      // ì˜¤ë””ì˜¤ ì¬ìƒì„ ìœ„í•œ AudioContext ìƒì„±
      const createPlaybackContext = () => {
        if (!playbackAudioContextRef.current) {
          playbackAudioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)();
        }
        return playbackAudioContextRef.current;
      };

      // ì—°ì†ì ì¸ ì˜¤ë””ì˜¤ ìŠ¤ì¼€ì¤„ë§ í•¨ìˆ˜
      const scheduleAudioChunk = async (pcm16Data: ArrayBuffer) => {
        try {
          const audioContext = createPlaybackContext();

          // PCM16 ë°ì´í„°ë¥¼ AudioBufferë¡œ ë³€í™˜
          const audioBuffer = audioContext.createBuffer(1, pcm16Data.byteLength / 2, 24000);
          const channelData = audioBuffer.getChannelData(0);
          const int16Array = new Int16Array(pcm16Data);

          // Int16ì„ Float32ë¡œ ë³€í™˜í•˜ê³  80% ì†ë„ ì ìš©
          for (let i = 0; i < int16Array.length; i++) {
            channelData[i] = int16Array[i] / 32767.0;
          }

          // AudioBufferSourceNode ìƒì„±
          const source = audioContext.createBufferSource();
          source.buffer = audioBuffer;
          source.connect(audioContext.destination);

          // ì‹œì‘ ì‹œê°„ ê³„ì‚° - ì´ì „ ì²­í¬ì˜ ëì— ì´ì–´ì„œ ì¬ìƒ
          const currentTime = audioContext.currentTime;
          const startTime = Math.max(currentTime, nextStartTimeRef.current);

          // ë‹¤ìŒ ì²­í¬ì˜ ì‹œì‘ ì‹œê°„ì„ ì„¤ì • (ì •ìƒ ì†ë„)
          const chunkDuration = audioBuffer.duration; // ì •ìƒ ì†ë„ë¡œ ì¬ìƒì‹œ ì‹¤ì œ ì†Œìš” ì‹œê°„
          nextStartTimeRef.current = startTime + chunkDuration;

          // ì²« ë²ˆì§¸ ì²­í¬ê°€ ì•„ë‹ˆë¼ë©´ ì•½ê°„ì˜ ì˜¤ë²„ë©ì„ ì¶”ê°€í•´ì„œ ëŠê¹€ ë°©ì§€
          const actualStartTime = scheduledSourcesRef.current.length > 0 ?
            startTime - 0.01 : // 10ms ì˜¤ë²„ë©
            startTime;

          scheduledSourcesRef.current.push(source);

          source.onended = () => {
            console.log('âœ… Audio chunk finished');
            // ì™„ë£Œëœ sourceë¥¼ ë°°ì—´ì—ì„œ ì œê±°
            const index = scheduledSourcesRef.current.indexOf(source);
            if (index > -1) {
              scheduledSourcesRef.current.splice(index, 1);
            }
          };

          source.playbackRate.value = 1.0; // ì •ìƒ ì†ë„
          source.start(actualStartTime);

          console.log(`âœ… Audio chunk scheduled at ${actualStartTime.toFixed(3)}s, duration: ${chunkDuration.toFixed(3)}s`);

        } catch (error) {
          console.error('âŒ Audio scheduling failed:', error);
        }
      };

      // ì˜¤ë””ì˜¤ ì´ë²¤íŠ¸ ì²˜ë¦¬ - ì—°ì† ìŠ¤íŠ¸ë¦¬ë° + ì„ì‹œ ì „ì‚¬ ë©”ì‹œì§€ ì¶”ê°€
      session.on('audio', (audioEvent) => {
        console.log('ğŸ”Š Audio event received from Agent SDK:', audioEvent);
        if (audioEvent && audioEvent.data) {
          console.log('ğŸµ Scheduling audio chunk, size:', audioEvent.data.byteLength);
          scheduleAudioChunk(audioEvent.data);

          // AIê°€ ì‘ë‹µì„ ì‹œì‘í–ˆë‹¤ë©´ ì„ì‹œ ë©”ì‹œì§€ ì¶”ê°€
          if (!currentAssistantTranscript) {
            console.log('ğŸ¤– AI started speaking, adding placeholder message');
            const assistantMessage: ChatMessage = {
              id: `assistant-${Date.now()}`,
              role: 'assistant',
              content: '[AIê°€ ì‘ë‹µ ì¤‘...]',
              timestamp: Date.now(),
              type: 'audio'
            };
            setMessages(prev => {
              // ì´ë¯¸ placeholderê°€ ìˆëŠ”ì§€ í™•ì¸
              const hasPlaceholder = prev.some(msg =>
                msg.role === 'assistant' && msg.content === '[AIê°€ ì‘ë‹µ ì¤‘...]'
              );
              if (!hasPlaceholder) {
                return [...prev, assistantMessage];
              }
              return prev;
            });
            setCurrentAssistantTranscript('[AIê°€ ì‘ë‹µ ì¤‘...]');
          }
        } else {
          console.log('âš ï¸ No audio data in event');
        }
      });

      // ì˜¤ë””ì˜¤ ì¤‘ë‹¨ ì²˜ë¦¬ - ëª¨ë“  ìŠ¤ì¼€ì¤„ëœ ì²­í¬ ì¤‘ì§€
      session.on('audio_interrupted', () => {
        console.log('ğŸ›‘ Audio interrupted - stopping all scheduled chunks');

        // ëª¨ë“  ìŠ¤ì¼€ì¤„ëœ ì˜¤ë””ì˜¤ ì†ŒìŠ¤ ì¤‘ì§€
        scheduledSourcesRef.current.forEach(source => {
          try {
            source.stop();
          } catch (error) {
            // ì´ë¯¸ ì¤‘ì§€ëœ ì†ŒìŠ¤ì¼ ìˆ˜ ìˆìŒ
          }
        });

        // ì°¸ì¡° ì´ˆê¸°í™”
        scheduledSourcesRef.current = [];
        nextStartTimeRef.current = 0;

        // AI ì‘ë‹µ ì™„ë£Œ ì²˜ë¦¬ - ì‹¤ì œ ë²ˆì—­ëœ ë‚´ìš©ìœ¼ë¡œ ì¶”ì •
        console.log('ğŸ¤– AI response completed, updating placeholder message');
        setMessages(prev => {
          return prev.map(msg => {
            if (msg.role === 'assistant' && msg.content === '[AIê°€ ì‘ë‹µ ì¤‘...]') {
              // ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ì…ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ë²ˆì—­ ë‚´ìš© ì¶”ì •
              const lastUserMessage = prev.filter(m => m.role === 'user').pop();
              let translatedContent = `${customerLanguage} ë²ˆì—­ ì™„ë£Œ`;

              if (lastUserMessage && lastUserMessage.content !== '[ì‚¬ìš©ì ìŒì„± ì…ë ¥ ì¤‘...]') {
                // ê°„ë‹¨í•œ ë²ˆì—­ ì˜ˆì‹œ (ì‹¤ì œ ë²ˆì—­ì´ ì•„ë‹Œ í‘œì‹œìš©)
                if (customerLanguage === 'English') {
                  translatedContent = `"${lastUserMessage.content}"ì˜ ì˜ì–´ ë²ˆì—­ì´ ìŒì„±ìœ¼ë¡œ ì¬ìƒë˜ì—ˆìŠµë‹ˆë‹¤`;
                } else if (customerLanguage === 'Chinese') {
                  translatedContent = `"${lastUserMessage.content}"ì˜ ì¤‘êµ­ì–´ ë²ˆì—­ì´ ìŒì„±ìœ¼ë¡œ ì¬ìƒë˜ì—ˆìŠµë‹ˆë‹¤`;
                } else {
                  translatedContent = `"${lastUserMessage.content}"ì˜ ${customerLanguage} ë²ˆì—­ì´ ìŒì„±ìœ¼ë¡œ ì¬ìƒë˜ì—ˆìŠµë‹ˆë‹¤`;
                }
              }

              return {
                ...msg,
                content: translatedContent,
                timestamp: Date.now()
              };
            }
            return msg;
          });
        });
        setCurrentAssistantTranscript('');

        console.log('âœ… All audio sources stopped');
      });

      // ì‚¬ìš©ì ìŒì„± ì…ë ¥ ì „ì‚¬ ì™„ë£Œ ì´ë²¤íŠ¸ ì²˜ë¦¬ (ì˜¬ë°”ë¥¸ ì´ë²¤íŠ¸ëª…)
      session.on('conversation.item.input_audio_transcription.completed', (event: any) => {
        console.log('ğŸ¤ User audio transcription completed:', JSON.stringify(event, null, 2));
        if (event && event.transcript && event.transcript.trim()) {
          const userMessage: ChatMessage = {
            id: `user-${Date.now()}-${Math.random()}`,
            role: 'user',
            content: event.transcript.trim(),
            timestamp: Date.now(),
            type: 'audio'
          };
          console.log('ğŸ’¬ Adding user transcription to chat:', userMessage);
          setMessages(prev => {
            console.log('ğŸ“Š Current messages before adding user:', prev.length);
            // ì¤‘ë³µ ë°©ì§€: ê°™ì€ ë‚´ìš©ì˜ ë©”ì‹œì§€ê°€ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
            const isDuplicate = prev.some(msg =>
              msg.role === 'user' &&
              msg.content === userMessage.content &&
              Math.abs(msg.timestamp - userMessage.timestamp) < 2000 // 2ì´ˆ ë‚´
            );
            if (isDuplicate) {
              console.log('ğŸ’¬ Duplicate user message, skipping');
              return prev;
            }
            const newMessages = [...prev, userMessage];
            console.log('ğŸ“Š New messages after adding user:', newMessages.length);
            return newMessages;
          });
        } else {
          console.log('âš ï¸ No valid transcript in user audio event:', event);
        }
      });

      // AI ì‘ë‹µ í…ìŠ¤íŠ¸ ì‹¤ì‹œê°„ ì²˜ë¦¬ - response.output_text.deltaë¡œ í† í° ë‹¨ìœ„ ìˆ˜ì‹ 

      // ê¸°ì¡´ output_text ì´ë²¤íŠ¸ ì œê±° - rawEventë¡œ ëŒ€ì²´ë¨

      // rawEvent ì‚¬ìš©ìœ¼ë¡œ RealtimeAgentì˜ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
      let rawCurrentAssistantMessageId: string | undefined;
      let assistantTextBuffer = "";

      // ì‹¤ì œ ì´ë²¤íŠ¸ êµ¬ì¡° ë””ë²„ê¹… ë° ì˜¬ë°”ë¥¸ ì´ë²¤íŠ¸ ì²˜ë¦¬
      console.log('ğŸ” ëª¨ë“  ì´ë²¤íŠ¸ ë””ë²„ê¹… ì‹œì‘');

      // ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ì´ë²¤íŠ¸ íƒ€ì…ë“¤ì„ ë¦¬ìŠ¤ë‹
      const allEventTypes = [
        'response.output_text.delta',
        'response.output_text.done',
        'response.text.delta',
        'response.text.done',
        'response.audio_transcript.delta',
        'response.audio_transcript.done',
        'response.content_part.added',
        'response.content_part.done',
        'conversation.item.created',
        'response.created',
        'response.done',
        'conversation.item.input_audio_transcription.completed',
        'conversation.item.input_audio_transcription.failed'
      ];

      allEventTypes.forEach(eventType => {
        session.on(eventType as any, (event: any) => {
          console.log(`ğŸ”” ì´ë²¤íŠ¸ ìˆ˜ì‹ : ${eventType}`, event);

          // AI ì‘ë‹µ í…ìŠ¤íŠ¸ ë¸íƒ€ ì²˜ë¦¬
          if (eventType === 'response.output_text.delta' || eventType === 'response.text.delta') {
            if (event && event.delta) {
              console.log('âœ… í…ìŠ¤íŠ¸ ë¸íƒ€ ì²˜ë¦¬:', event.delta);
              assistantTextBuffer += event.delta;
              rawCurrentAssistantMessageId = addOrUpdateAssistantMessage(assistantTextBuffer, rawCurrentAssistantMessageId);
            }
          }

          // AI ì‘ë‹µ ì˜¤ë””ì˜¤ ì „ì‚¬ ë¸íƒ€ ì²˜ë¦¬ (ëŒ€ì•ˆ)
          if (eventType === 'response.audio_transcript.delta') {
            if (event && event.delta) {
              console.log('âœ… ì˜¤ë””ì˜¤ ì „ì‚¬ ë¸íƒ€ ì²˜ë¦¬:', event.delta);
              assistantTextBuffer += event.delta;
              rawCurrentAssistantMessageId = addOrUpdateAssistantMessage(assistantTextBuffer, rawCurrentAssistantMessageId);
            }
          }

          // ì‘ë‹µ ì™„ë£Œ ì²˜ë¦¬
          if (eventType === 'response.done' || eventType === 'response.output_text.done' || eventType === 'response.audio_transcript.done') {
            console.log('âœ… ì‘ë‹µ ì™„ë£Œ ì²˜ë¦¬');
            assistantTextBuffer = "";
            rawCurrentAssistantMessageId = undefined;
            setCurrentAssistantTranscript('');
          }

          // ì‚¬ìš©ì ìŒì„± ì „ì‚¬ ì²˜ë¦¬
          if (eventType === 'conversation.item.input_audio_transcription.completed') {
            if (event && event.transcript && event.transcript.trim()) {
              console.log('âœ… ì‚¬ìš©ì ì „ì‚¬ ì²˜ë¦¬:', event.transcript);
              const userMessage: ChatMessage = {
                id: `user-${Date.now()}-${Math.random()}`,
                role: 'user',
                content: event.transcript.trim(),
                timestamp: Date.now(),
                type: 'audio'
              };
              setMessages(prev => {
                const withoutPlaceholder = prev.filter(msg =>
                  !(msg.role === 'user' && msg.content === '[ì‚¬ìš©ì ìŒì„± ì…ë ¥ ì¤‘...]')
                );
                return [...withoutPlaceholder, userMessage];
              });
            }
          }
        });
      });

      // ì¶”ê°€ì ìœ¼ë¡œ rawEventë„ ì‹œë„
      if (typeof (session as any).on === 'function') {
        try {
          (session as any).on('rawEvent', (event: any) => {
            console.log('ğŸ” rawEvent ë°œê²¬:', event.type, event);
          });
        } catch (e) {
          console.log('ğŸ” rawEvent ì§€ì›ë˜ì§€ ì•ŠìŒ');
        }
      }

      // ë„ìš°ë¯¸ í•¨ìˆ˜: assistant ë©”ì‹œì§€ ì¶”ê°€/ì—…ë°ì´íŠ¸ (íƒ€ìì¹˜ëŠ” íš¨ê³¼)
      const addOrUpdateAssistantMessage = (content: string, messageId?: string): string => {
        let newMessageId = messageId;

        console.log('ğŸ“ ë©”ì‹œì§€ ì—…ë°ì´íŠ¸:', { content, messageId });

        setMessages(prev => {
          console.log('ğŸ“Š í˜„ì¬ ë©”ì‹œì§€ ê°œìˆ˜:', prev.length);

          // placeholderë¥¼ ì°¾ì•„ì„œ êµì²´í•˜ê±°ë‚˜ ê¸°ì¡´ ë©”ì‹œì§€ ì—…ë°ì´íŠ¸
          const placeholderIndex = prev.findIndex(msg =>
            msg.role === 'assistant' &&
            (msg.content === '[AIê°€ ì‘ë‹µ ì¤‘...]' || msg.id === messageId)
          );

          if (placeholderIndex !== -1) {
            console.log('âœ… placeholder/ê¸°ì¡´ ë©”ì‹œì§€ ì—…ë°ì´íŠ¸:', placeholderIndex);
            // ê¸°ì¡´ ë©”ì‹œì§€ ì—…ë°ì´íŠ¸
            const updatedMessage = {
              ...prev[placeholderIndex],
              content: content,
              timestamp: Date.now()
            };
            newMessageId = updatedMessage.id;
            return [
              ...prev.slice(0, placeholderIndex),
              updatedMessage,
              ...prev.slice(placeholderIndex + 1)
            ];
          } else {
            console.log('âœ… ìƒˆ assistant ë©”ì‹œì§€ ìƒì„±');
            // ìƒˆ ë©”ì‹œì§€ ìƒì„±
            const assistantMessage: ChatMessage = {
              id: `assistant-${Date.now()}-${Math.random()}`,
              role: 'assistant',
              content: content,
              timestamp: Date.now(),
              type: 'audio'
            };
            newMessageId = assistantMessage.id;
            const newMessages = [...prev, assistantMessage];
            console.log('ğŸ“Š ìƒˆ ë©”ì‹œì§€ ì¶”ê°€ í›„ ê°œìˆ˜:', newMessages.length);
            return newMessages;
          }
        });

        return newMessageId || `assistant-${Date.now()}-${Math.random()}`;
      };

      // ëª¨ë“  ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆë“¤
      session.on('audio_interrupted', () => {
        console.log('ğŸ›‘ Audio interrupted');
      });

      session.on('error', (error) => {
        console.error('âŒ Session error:', error);
        console.error('âŒ Error details:', JSON.stringify(error, null, 2));
        if (error && typeof error === 'object' && 'error' in error) {
          console.error('âŒ Inner error:', (error as any).error);
        }
        setError('ì„¸ì…˜ ì˜¤ë¥˜: ' + (error && typeof error === 'object' && 'message' in error ? (error as any).message : JSON.stringify(error)));
      });

      // history_updated ì´ë²¤íŠ¸ë¡œ í™•ì‹¤í•˜ê²Œ ë©”ì‹œì§€ ë°›ê¸°
      session.on('history_updated', (history: any[]) => {
        console.log('ğŸ“ History updated from Agent SDK:', history.length, 'items');
        const newMessages: ChatMessage[] = [];

        for (const item of history) {
          console.log('ğŸ“‹ Processing history item:', item.type, item);

          if (item.type === 'message' && item.role && item.content) {
            const content = Array.isArray(item.content)
              ? item.content.map((c: any) => c.type === 'text' ? c.text : '').join('')
              : item.content || '';

            if (typeof content === 'string' && content.trim()) {
              const message: ChatMessage = {
                id: `${item.role}-${Date.now()}-${Math.random()}`,
                role: item.role,
                content: content.trim(),
                timestamp: Date.now(),
                type: 'audio'
              };

              console.log('ğŸ’¬ Adding message to history:', message);
              newMessages.push(message);
            }
          }
        }

        console.log('ğŸ“¨ Setting messages:', newMessages.length, 'total messages');
        setMessages(newMessages);
      });

      // ëª¨ë“  ê°€ëŠ¥í•œ ì´ë²¤íŠ¸ë“¤ ë¦¬ìŠ¤ë‹
      const eventTypes = [
        'connected', 'disconnected', 'response',
        'input_audio_buffer.speech_started', 'input_audio_buffer.speech_stopped',
        'input_audio_buffer.committed', 'conversation.item.created',
        'response.created', 'response.done', 'response.output_item.added',
        'response.content_part.added', 'response.audio_transcript.done',
        'response.audio_transcript.delta',
        'session.created', 'session.updated',
        'conversation.item.input_audio_transcription.completed',
        'history_updated'
      ];
      eventTypes.forEach(eventType => {
        session.on(eventType as any, (...args: any[]) => {
          console.log(`ğŸ”” Event: ${eventType}`, args);
        });
      });

      // ì—°ê²° (transportì—ì„œ ì´ë¯¸ useInsecureApiKey ì„¤ì •ë¨)
      await session.connect({ apiKey });
      sessionRef.current = session;
      setIsConnected(true);
      console.log('âœ… Agent connected successfully');

    } catch (err) {
      console.error('âŒ Connection failed:', err);
      setError(err instanceof Error ? err.message : 'Connection failed');
    } finally {
      setIsConnecting(false);
    }
  }, [createAgent, isConnected, isConnecting]);

  // Web Speech API ì„¤ì •
  const setupSpeechRecognition = useCallback(() => {
    if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
      console.log('âš ï¸ Speech Recognition not supported');
      return null;
    }

    const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
    const recognition = new SpeechRecognition();

    recognition.continuous = true;
    recognition.interimResults = true;
    recognition.lang = 'ko-KR'; // í•œêµ­ì–´ ì„¤ì •

    recognition.onstart = () => {
      console.log('ğŸ¤ Speech recognition started');
    };

    recognition.onresult = (event: any) => {
      let finalTranscript = '';
      let interimTranscript = '';

      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;
        if (event.results[i].isFinal) {
          finalTranscript += transcript;
        } else {
          interimTranscript += transcript;
        }
      }

      console.log('ğŸ¤ Speech result:', { final: finalTranscript, interim: interimTranscript });

      if (finalTranscript) {
        // ìµœì¢… ê²°ê³¼ë¥¼ ì±„íŒ…ì— ì¶”ê°€
        const userMessage: ChatMessage = {
          id: `user-${Date.now()}-${Math.random()}`,
          role: 'user',
          content: finalTranscript.trim(),
          timestamp: Date.now(),
          type: 'audio'
        };

        setMessages(prev => {
          // placeholder ë©”ì‹œì§€ë¥¼ ì‹¤ì œ ì „ì‚¬ë¡œ êµì²´
          const withoutPlaceholder = prev.filter(msg =>
            !(msg.role === 'user' && msg.content === '[ì‚¬ìš©ì ìŒì„± ì…ë ¥ ì¤‘...]')
          );
          return [...withoutPlaceholder, userMessage];
        });

        setCurrentUserTranscript(finalTranscript);
        setIsUserSpeaking(false);
      } else if (interimTranscript) {
        // ì„ì‹œ ê²°ê³¼ë¡œ placeholder ì—…ë°ì´íŠ¸
        setMessages(prev => {
          return prev.map(msg => {
            if (msg.role === 'user' && msg.content === '[ì‚¬ìš©ì ìŒì„± ì…ë ¥ ì¤‘...]') {
              return { ...msg, content: `ğŸ¤ ${interimTranscript}` };
            }
            return msg;
          });
        });
      }
    };

    recognition.onerror = (event: any) => {
      console.error('âŒ Speech recognition error:', event.error);
    };

    recognition.onend = () => {
      console.log('ğŸ¤ Speech recognition ended');
    };

    return recognition;
  }, []);

  // ìŒì„± ë…¹ìŒ ì‹œì‘ (ì—°ì† ëª¨ë“œ)
  const startRecording = useCallback(async () => {
    if (isRecording) {
      console.log('âš ï¸ Already recording, ignoring start request');
      return;
    }

    setError(null); // ì´ì „ ì˜¤ë¥˜ ì´ˆê¸°í™”

    try {
      // ì—°ê²°ë˜ì§€ ì•Šì€ ê²½ìš° ë¨¼ì € ì—°ê²°
      if (!isConnected) {
        console.log('ğŸ”„ Connecting to Agent SDK...');
        await connect();

        // ì„¸ì…˜ì´ ì™„ì „íˆ ì¤€ë¹„ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼
        let attempts = 0;
        while (!sessionRef.current && attempts < 30) { // ìµœëŒ€ 3ì´ˆ ëŒ€ê¸°
          await new Promise(resolve => setTimeout(resolve, 100));
          attempts++;
        }

        if (!sessionRef.current) {
          throw new Error('Session failed to initialize after connection');
        }

        console.log('âœ… Session ready for recording');
      }

      if (!sessionRef.current) {
        throw new Error('Session not ready - please try again');
      }

      // ë§ˆì´í¬ í…ŒìŠ¤íŠ¸ ë¨¼ì € ìˆ˜í–‰
      console.log('ğŸ¤ Testing microphone access...');
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        throw new Error('MediaDevices API not supported in this browser');
      }

      // ì‚¬ìš© ê°€ëŠ¥í•œ ì˜¤ë””ì˜¤ ì¥ì¹˜ í™•ì¸
      const devices = await navigator.mediaDevices.enumerateDevices();
      const audioInputs = devices.filter(device => device.kind === 'audioinput');
      console.log('ğŸ¤ Available audio input devices:', audioInputs.length, audioInputs);

      if (audioInputs.length === 0) {
        throw new Error('No audio input devices found');
      }

      // ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­ ë° ìŠ¤íŠ¸ë¦¼ íšë“
      console.log('ğŸ¤ Requesting microphone access...');
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 24000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: false, // Agent SDKì—ì„œ ì§ì ‘ ì²˜ë¦¬
        }
      });

      console.log('âœ… Microphone access granted');
      console.log('ğŸ¤ Stream details:', {
        active: stream.active,
        tracks: stream.getTracks().length,
        audioTracks: stream.getAudioTracks().map(track => ({
          enabled: track.enabled,
          muted: track.muted,
          readyState: track.readyState,
          label: track.label
        }))
      });

      streamRef.current = stream;

      // AudioContext ìƒì„± (24kHzë¡œ ì„¤ì •)
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)({
        sampleRate: 24000
      });
      audioContextRef.current = audioContext;

      console.log('ğŸ”§ AudioContext created:', {
        state: audioContext.state,
        sampleRate: audioContext.sampleRate,
        currentTime: audioContext.currentTime
      });

      // AudioContextê°€ suspended ìƒíƒœë¼ë©´ resume
      if (audioContext.state === 'suspended') {
        console.log('ğŸ”§ AudioContext is suspended, resuming...');
        await audioContext.resume();
        console.log('âœ… AudioContext resumed, state:', audioContext.state);
      }

      try {
        console.log('ğŸ”§ Loading AudioWorklet module...');
        await audioContext.audioWorklet.addModule('/audio-processor.js');
        console.log('âœ… AudioWorklet module loaded');

        // ë§ˆì´í¬ ì…ë ¥ì„ AudioContextì— ì—°ê²°
        const source = audioContext.createMediaStreamSource(stream);
        console.log('âœ… MediaStreamSource created');

        // AudioWorkletNode ìƒì„±
        console.log('ğŸ”§ Creating AudioWorkletNode...');
        const workletNode = new AudioWorkletNode(audioContext, 'pcm16-processor');
        workletNodeRef.current = workletNode;
        console.log('âœ… AudioWorkletNode created');

        // ì›Œí¬ë ›ì—ì„œ ì˜¤ëŠ” ì˜¤ë””ì˜¤ ë°ì´í„° ì²˜ë¦¬
        console.log('ğŸ”§ Setting up workletNode.port.onmessage handler...');
        workletNode.port.onmessage = (event) => {
          if (event.data.type === 'audioData' && sessionRef.current) {
            const { buffer, level, samples, count } = event.data;

            // ì²« ë²ˆì§¸ ì˜¤ë””ì˜¤ ë°ì´í„° ìˆ˜ì‹  ì‹œ ì•Œë¦¼
            if (count === 1) {
              console.log('ğŸ‰ First audio data received from AudioWorklet!');
            }

            // ì‚¬ìš©ì ìŒì„± ê°ì§€ ë° placeholder ë©”ì‹œì§€ ì¶”ê°€
            const significantAudio = level > 0.02; // 2% ì´ìƒ
            if (significantAudio && !isUserSpeaking) {
              console.log('ğŸ¤ User started speaking, adding placeholder message');
              setIsUserSpeaking(true);
              const userMessage: ChatMessage = {
                id: `user-${Date.now()}`,
                role: 'user',
                content: '[ì‚¬ìš©ì ìŒì„± ì…ë ¥ ì¤‘...]',
                timestamp: Date.now(),
                type: 'audio'
              };
              setMessages(prev => {
                // ì´ë¯¸ placeholderê°€ ìˆëŠ”ì§€ í™•ì¸
                const hasPlaceholder = prev.some(msg =>
                  msg.role === 'user' && msg.content === '[ì‚¬ìš©ì ìŒì„± ì…ë ¥ ì¤‘...]'
                );
                if (!hasPlaceholder) {
                  return [...prev, userMessage];
                }
                return prev;
              });
              setCurrentUserTranscript('[ì‚¬ìš©ì ìŒì„± ì…ë ¥ ì¤‘...]');
            }

            // ìŒì„±ì´ ì•½í•´ì§€ë©´ ì‚¬ìš©ì ë°œí™” ì¢…ë£Œë¡œ ê°„ì£¼
            if (!significantAudio && isUserSpeaking && count % 50 === 0) {
              console.log('ğŸ¤ User might have stopped speaking (low level detected)');
              // 2ì´ˆ í›„ì— placeholder ì œê±°í•˜ë„ë¡ ì§€ì—°
              setTimeout(() => {
                setIsUserSpeaking(false);
                setCurrentUserTranscript('');
              }, 2000);
            }

            // ì˜¤ë””ì˜¤ ë ˆë²¨ ë¡œê¹… (ë§¤ 50ë²ˆì§¸ë§ˆë‹¤ - ë” ìì£¼ í‘œì‹œ)
            if (count % 50 === 0) {
              console.log('ğŸ”Š Audio level:', (level * 100).toFixed(1) + '%', 'samples:', samples, 'count:', count);
            }

            // ì˜¤ë””ì˜¤ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ê²½ê³ 
            if (level < 0.001 && count % 100 === 0) {
              console.warn('âš ï¸ Very low audio level detected. Check microphone volume.');
            }

            // ìœ ì˜ë¯¸í•œ ì˜¤ë””ì˜¤ ë ˆë²¨ì´ ìˆì„ ë•Œë§Œ ìƒì„¸ ë¡œê¹…
            if (level > 0.01) {
              console.log('ğŸ¤ Sending PCM16 audio:', buffer.byteLength, 'bytes', 'level:', (level * 100).toFixed(1) + '%');
            }

            try {
              // Agent SDKê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ì „ì†¡
              sessionRef.current.sendAudio(buffer);

              // ì „ì†¡ í†µê³„ ì¶”ì 
              if (count === 1) {
                console.log('ğŸ“¡ Started sending audio to Agent SDK');
              }

              if (level > 0.01) { // ìœ ì˜ë¯¸í•œ ì˜¤ë””ì˜¤ë§Œ ì„±ê³µ ë¡œê·¸
                console.log('âœ… PCM16 audio sent successfully, level:', (level * 100).toFixed(1) + '%', 'count:', count);
              }

              // ë§¤ 100ë²ˆì§¸ë§ˆë‹¤ ì „ì†¡ ìƒíƒœ í™•ì¸
              if (count % 100 === 0) {
                console.log('ğŸ“Š Audio transmission stats - Count:', count, 'Recent level:', (level * 100).toFixed(2) + '%');
              }
            } catch (error) {
              console.error('âŒ Failed to send PCM16 audio:', error);
            }
          }
        };
        console.log('âœ… Message handler set up');

        // ì˜¤ë””ì˜¤ ê·¸ë˜í”„ ì—°ê²°
        console.log('ğŸ”§ Connecting audio graph...');
        console.log('ğŸ”§ Source node:', source);
        console.log('ğŸ”§ Worklet node:', workletNode);

        source.connect(workletNode);
        console.log('âœ… Source connected to worklet');

        // AudioWorkletNodeê°€ ì²˜ë¦¬ë˜ë ¤ë©´ destinationì—ë„ ì—°ê²°í•´ì•¼ í•  ìˆ˜ ìˆìŒ
        workletNode.connect(audioContext.destination);
        console.log('âœ… Worklet connected to destination');

        console.log('âœ… Complete audio graph: source â†’ workletNode â†’ destination');

      } catch (workletError) {
        console.warn('âš ï¸ AudioWorklet failed:', workletError);
        console.warn('ğŸ”„ Falling back to ScriptProcessorNode for compatibility');

        // Fallback to ScriptProcessorNode for compatibility
        const source = audioContext.createMediaStreamSource(stream);
        const processor = audioContext.createScriptProcessor(4096, 1, 1);
        console.log('âœ… ScriptProcessorNode created (fallback)');

        let fallbackCount = 0;
        processor.onaudioprocess = (event) => {
          if (!sessionRef.current) return;

          fallbackCount++;
          const inputBuffer = event.inputBuffer;
          const inputData = inputBuffer.getChannelData(0);

          // ì˜¤ë””ì˜¤ ë ˆë²¨ ê³„ì‚° (RMS)
          let sum = 0;
          for (let i = 0; i < inputData.length; i++) {
            sum += inputData[i] * inputData[i];
          }
          const rms = Math.sqrt(sum / inputData.length);
          const level = Math.max(0, Math.min(1, rms));

          // ì²« ë²ˆì§¸ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì‹œ ì•Œë¦¼
          if (fallbackCount === 1) {
            console.log('ğŸ‰ First audio data processed with ScriptProcessorNode!');
          }

          // ì£¼ê¸°ì  ë ˆë²¨ ë¡œê¹…
          if (fallbackCount % 50 === 0) {
            console.log('ğŸ”Š Audio level (fallback):', (level * 100).toFixed(1) + '%', 'count:', fallbackCount);
          }

          const pcm16Buffer = new Int16Array(inputData.length);
          for (let i = 0; i < inputData.length; i++) {
            const sample = Math.max(-1, Math.min(1, inputData[i]));
            pcm16Buffer[i] = sample * 0x7FFF;
          }

          try {
            sessionRef.current.sendAudio(pcm16Buffer.buffer);
            if (level > 0.01) { // ìœ ì˜ë¯¸í•œ ì˜¤ë””ì˜¤ë§Œ ì„±ê³µ ë¡œê·¸
              console.log('âœ… PCM16 audio sent (fallback), level:', (level * 100).toFixed(1) + '%');
            }
          } catch (error) {
            console.error('âŒ Failed to send PCM16 audio (fallback):', error);
          }
        };

        source.connect(processor);
        processor.connect(audioContext.destination);
        console.log('âœ… Audio processing connected (ScriptProcessorNode fallback)');
      }

      // Web Speech API ì‹œì‘
      const recognition = setupSpeechRecognition();
      if (recognition) {
        setSpeechRecognition(recognition);
        recognition.start();
        console.log('ğŸ¤ Speech recognition started along with recording');
      }

      setIsRecording(true);
      console.log('ğŸ¤ Recording started - continuous PCM16 streaming');

    } catch (err) {
      console.error('âŒ Recording failed:', err);
      if (err instanceof Error) {
        if (err.name === 'NotAllowedError') {
          setError('ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤. ë¸Œë¼ìš°ì €ì—ì„œ ë§ˆì´í¬ ì ‘ê·¼ì„ í—ˆìš©í•´ì£¼ì„¸ìš”.');
        } else if (err.name === 'NotFoundError') {
          setError('ë§ˆì´í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§ˆì´í¬ê°€ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.');
        } else {
          setError(`ë…¹ìŒ ì˜¤ë¥˜: ${err.message}`);
        }
      } else {
        setError('ë…¹ìŒì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
      }
    }
  }, [isRecording, isConnected, connect]);

  // ìŒì„± ë…¹ìŒ ì¤‘ì§€ ë° ì„¸ì…˜ ì¢…ë£Œ
  const stopRecording = useCallback(() => {
    console.log('ğŸ›‘ Stopping recording and terminating session...');

    // Speech Recognition ì¤‘ì§€
    if (speechRecognition) {
      speechRecognition.stop();
      setSpeechRecognition(null);
      console.log('ğŸ¤ Speech recognition stopped');
    }

    // ëª¨ë“  ìŠ¤ì¼€ì¤„ëœ ì˜¤ë””ì˜¤ ì†ŒìŠ¤ ì¤‘ì§€
    scheduledSourcesRef.current.forEach(source => {
      try {
        source.stop();
      } catch (error) {
        // ì´ë¯¸ ì¤‘ì§€ëœ ì†ŒìŠ¤ì¼ ìˆ˜ ìˆìŒ
      }
    });
    scheduledSourcesRef.current = [];
    nextStartTimeRef.current = 0;

    // ì¬ìƒìš© AudioContext ì •ë¦¬
    if (playbackAudioContextRef.current) {
      playbackAudioContextRef.current.close();
      playbackAudioContextRef.current = null;
    }

    // AudioWorklet ì •ë¦¬
    if (workletNodeRef.current) {
      workletNodeRef.current.disconnect();
      workletNodeRef.current = null;
    }

    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }

    // ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }

    // ì„¸ì…˜ ì¢…ë£Œ
    if (sessionRef.current) {
      sessionRef.current = null;
    }

    // ì˜¤ë””ì˜¤ ì²­í¬ ì •ë¦¬
    audioChunksRef.current = [];
    isPlayingAudioRef.current = false;

    setIsRecording(false);
    setIsConnected(false);
    setIsUserSpeaking(false);
    setCurrentUserTranscript('');
    setCurrentAssistantTranscript('');
    setMessages([]); // ëŒ€í™” ë‚´ì—­ ì´ˆê¸°í™”
    console.log('âœ… Recording stopped and session terminated');
  }, []);

  // ì—°ê²° í•´ì œ
  const disconnect = useCallback(() => {
    stopRecording();

    if (sessionRef.current) {
      sessionRef.current = null;
    }

    setIsConnected(false);
    setMessages([]);
    console.log('ğŸ”Œ Disconnected');
  }, [stopRecording]);

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
  useEffect(() => {
    return () => {
      disconnect();
    };
  }, [disconnect]);

  return {
    isConnected,
    isRecording,
    messages,
    error,
    isConnecting,
    connect,
    disconnect,
    startRecording,
    stopRecording,
  };
}