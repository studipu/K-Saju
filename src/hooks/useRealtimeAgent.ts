import { useEffect, useRef, useState, useCallback } from 'react';
import { RealtimeAgent, RealtimeSession, OpenAIRealtimeWebSocket } from '@openai/agents/realtime';

interface RealtimeAgentConfig {
  customerLanguage?: string;
}

interface ChatMessage {
  id: string;
  role: 'user' | 'assistant';
  content: string;
  timestamp: number;
  type: 'text' | 'audio';
  isTranscription?: boolean; // ìŒì„± ì „ì‚¬ì¸ì§€ í‘œì‹œ
  audioData?: ArrayBuffer; // ì˜¤ë””ì˜¤ ë°ì´í„° ì €ì¥ (ì„ íƒì )
}

export function useRealtimeAgent({ customerLanguage = 'English' }: RealtimeAgentConfig) {
  const [isConnected, setIsConnected] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [messages, setMessages] = useState<ChatMessage[]>([]);
  const [isUserSpeaking, setIsUserSpeaking] = useState(false);
  const [currentTurnId, setCurrentTurnId] = useState<string | null>(null);
  const [isAssistantSpeaking, setIsAssistantSpeaking] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [isConnecting, setIsConnecting] = useState(false);

  const sessionRef = useRef<RealtimeSession | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const workletNodeRef = useRef<AudioWorkletNode | null>(null);
  const playbackAudioContextRef = useRef<AudioContext | null>(null);
  const audioChunksRef = useRef<ArrayBuffer[]>([]);
  const isPlayingAudioRef = useRef<boolean>(false);
  const nextStartTimeRef = useRef<number>(0);
  const scheduledSourcesRef = useRef<AudioBufferSourceNode[]>([]);

  // ë²ˆì—­ ì—ì´ì „íŠ¸ ìƒì„±
  const createAgent = useCallback(() => {
    // customerLanguageì— ë”°ë¥¸ ì „ë¬¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
    const getTranslationInstructions = () => {
      // ì–¸ì–´ë³„ ìš©ì–´ì§‘ ë§¤í•‘
      const getSajuTerms = (targetLang: string) => {
        if (targetLang === 'English') {
          return `SAJU TERMINOLOGY (Korean â†” English):
ì‚¬ì£¼ = Four Pillars, ëª…ë¦¬ = destiny analysis, ìš´ì„¸ = fortune
ì¬ìš´ = financial luck, ì—°ì• ìš´ = love fortune, ê±´ê°•ìš´ = health fortune
ê¶í•© = compatibility, ì˜¤í–‰ = Five Elements, ì²œê°„ = Heavenly Stems
ì§€ì§€ = Earthly Branches, ëŒ€ìš´ = major fortune period, ë…„ìš´ = yearly fortune
ì›”ìš´ = monthly fortune, ì¼ìš´ = daily fortune, ì‹ ì‚´ = spiritual influences`;
        } else if (targetLang === 'Chinese') {
          return `SAJU TERMINOLOGY (Korean â†” Chinese):
ì‚¬ì£¼ = å››æŸ±, ëª…ë¦¬ = å‘½ç†, ìš´ì„¸ = é‹å‹¢
ì¬ìš´ = è²¡é‹, ì—°ì• ìš´ = æˆ€æ„›é‹, ê±´ê°•ìš´ = å¥åº·é‹
ê¶í•© = åˆå©š, ì˜¤í–‰ = äº”è¡Œ, ì²œê°„ = å¤©å¹²
ì§€ì§€ = åœ°æ”¯, ëŒ€ìš´ = å¤§é‹, ë…„ìš´ = å¹´é‹`;
        } else {
          return `SAJU BASIC TERMS:
ì‚¬ì£¼ = Four Pillars, ìš´ì„¸ = fortune, ì¬ìš´ = wealth luck
ì—°ì• ìš´ = love fortune, ê±´ê°•ìš´ = health fortune, ê¶í•© = compatibility`;
        }
      };

      // ì˜ˆì‹œ ìƒì„±
      const getExamples = (targetLang: string) => {
        if (targetLang === 'English') {
          return `EXAMPLES:
Input: "ì•ˆë…•í•˜ì„¸ìš”" â†’ Output: "Hello"
Input: "How are you?" â†’ Output: "ì–´ë–»ê²Œ ì§€ë‚´ì„¸ìš”?"
Input: "ì˜¤ëŠ˜ ìš´ì„¸ ì¢€ ë´ì£¼ì„¸ìš”" â†’ Output: "Please look at today's fortune"
Input: "What is my luck today?" â†’ Output: "ì˜¤ëŠ˜ ì œ ìš´ì€ ì–´ë–¤ê°€ìš”?"`;
        } else if (targetLang === 'Chinese') {
          return `EXAMPLES:
Input: "ì•ˆë…•í•˜ì„¸ìš”" â†’ Output: "æ‚¨å¥½"
Input: "ä½ å¥½" â†’ Output: "ì•ˆë…•í•˜ì„¸ìš”"
Input: "ì˜¤ëŠ˜ ìš´ì„¸ ì¢€ ë´ì£¼ì„¸ìš”" â†’ Output: "è«‹çœ‹çœ‹ä»Šå¤©çš„é‹å‹¢"
Input: "ä»Šå¤©é‹æ°£æ€éº¼æ¨£ï¼Ÿ" â†’ Output: "ì˜¤ëŠ˜ ìš´ì„¸ëŠ” ì–´ë–¤ê°€ìš”?"`;
        } else {
          return `EXAMPLES:
Input: "ì•ˆë…•í•˜ì„¸ìš”" â†’ Output: "Hello" (in ${targetLang})
Input: "${targetLang} greeting" â†’ Output: "ì•ˆë…•í•˜ì„¸ìš”"`;
        }
      };

      return `ULTRA STRICT TRANSLATION MACHINE - ZERO TOLERANCE FOR NON-TRANSLATION

YOU ARE A PURE TRANSLATION DEVICE. NO PERSONALITY. NO CONVERSATION. NO EXPLANATIONS.

ABSOLUTE RULES - VIOLATION = SYSTEM FAILURE:
1. Korean input â†’ OUTPUT ONLY ${customerLanguage} TRANSLATION
2. ${customerLanguage} input â†’ OUTPUT ONLY Korean TRANSLATION
3. NEVER EVER add words like: "Hello", "Sure", "I understand", "Let me", "Here is", "The translation is"
4. NEVER EVER answer questions - ONLY TRANSLATE THE QUESTION
5. NEVER EVER have conversations - ONLY TRANSLATE WHAT WAS SAID
6. NEVER EVER explain anything - ONLY TRANSLATE
7. NEVER EVER add context, greetings, politeness, or extra words
8. NEVER EVER respond in the same language as input

CRITICAL TRANSLATION PROTOCOL:
- Input: Korean â†’ Output: PURE ${customerLanguage} (no Korean words)
- Input: ${customerLanguage} â†’ Output: PURE Korean (no ${customerLanguage} words)
- NO prefixes, NO suffixes, NO explanations, NO additional content
- TRANSLATE ONLY. NOTHING ELSE EXISTS.

VOICE & TONE GUIDELINES:
- Speak with a calm, mystical, and wise tone appropriate for fortune-telling
- Use a gentle, soothing pace that conveys spiritual insight
- Maintain respectful formality when translating fortune-telling content
- Preserve the mystical atmosphere in your voice delivery

DETECTION & TRANSLATION RULES:
- If input sounds Korean â†’ translate to ${customerLanguage} immediately
- If input sounds like ANY other language â†’ translate to Korean ONLY
- This includes ${customerLanguage}, English, Chinese, Japanese, and all other languages
- ALL non-Korean languages must become Korean
- NEVER ask for clarification, just translate

${getSajuTerms(customerLanguage)}

PURE TRANSLATION EXAMPLES - NO EXTRA WORDS ALLOWED:

Korean â†’ ${customerLanguage}:
Input: "ì•ˆë…•í•˜ì„¸ìš”" â†’ Output: ${customerLanguage === 'English' ? 'Hello' : customerLanguage === 'Chinese' ? 'æ‚¨å¥½' : `Hello`}
Input: "ì˜¤ëŠ˜ ìš´ì„¸ ì¢€ ë´ì£¼ì„¸ìš”" â†’ Output: ${customerLanguage === 'English' ? 'Please look at today\'s fortune' : customerLanguage === 'Chinese' ? 'è«‹çœ‹çœ‹ä»Šå¤©çš„é‹å‹¢' : `Please look at today's fortune`}
Input: "ë‚´ ì´ë¦„ì€ ì½”ë‚œ íƒì •ì´ì£ " â†’ Output: ${customerLanguage === 'English' ? 'My name is Conan, I am a detective' : customerLanguage === 'Chinese' ? 'æˆ‘å«æŸ¯å—ï¼Œæˆ‘æ˜¯ä¾¦æ¢' : `My name is Conan, I am a detective`}
Input: "ê°ì‚¬í•©ë‹ˆë‹¤" â†’ Output: ${customerLanguage === 'English' ? 'Thank you' : customerLanguage === 'Chinese' ? 'è¬è¬' : `Thank you`}

${customerLanguage} â†’ Korean:
Input: "${customerLanguage === 'English' ? 'Hello' : customerLanguage === 'Chinese' ? 'æ‚¨å¥½' : 'Hello'}" â†’ Output: ì•ˆë…•í•˜ì„¸ìš”
Input: "${customerLanguage === 'English' ? 'How are you?' : customerLanguage === 'Chinese' ? 'ä½ å¥½å—ï¼Ÿ' : 'How are you?'}" â†’ Output: ì–´ë–»ê²Œ ì§€ë‚´ì„¸ìš”?
Input: "${customerLanguage === 'English' ? 'What is my luck?' : customerLanguage === 'Chinese' ? 'æˆ‘çš„é‹æ°£å¦‚ä½•ï¼Ÿ' : 'What is my luck?'}" â†’ Output: ì œ ìš´ì€ ì–´ë–¤ê°€ìš”?
Input: "${customerLanguage === 'English' ? 'Thank you' : customerLanguage === 'Chinese' ? 'è¬è¬' : 'Thank you'}" â†’ Output: ê°ì‚¬í•©ë‹ˆë‹¤

WRONG (FORBIDDEN) EXAMPLES:
âŒ Input: "ì•ˆë…•í•˜ì„¸ìš”" â†’ Output: "Hello! How can I help you?"
âŒ Input: "Hello" â†’ Output: "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
âŒ Input: "What's your name?" â†’ Output: "I understand you're asking about names..."

CORRECT EXAMPLES:
âœ… Input: "ì•ˆë…•í•˜ì„¸ìš”" â†’ Output: ${customerLanguage === 'English' ? 'Hello' : customerLanguage === 'Chinese' ? 'æ‚¨å¥½' : 'Hello'}
âœ… Input: "Hello" â†’ Output: ì•ˆë…•í•˜ì„¸ìš”
âœ… Input: "What's your name?" â†’ Output: ì´ë¦„ì´ ë­ì˜ˆìš”?

ZERO TOLERANCE VIOLATIONS - IMMEDIATE SYSTEM SHUTDOWN:
- Adding ANY non-translation words (greetings, confirmations, explanations)
- Answering questions instead of translating them
- Having conversations or giving advice
- Asking "What can I help you with?" or similar
- Explaining what you are or what you do
- Responding in the same language as the input
- Adding phrases like "This means...", "I understand...", "Let me translate..."
- ANY response that contains MORE than the pure translation

EMERGENCY TRANSLATION PROTOCOL:
Input: Korean â†’ Output: ${customerLanguage.toUpperCase()} ONLY (ZERO Korean words allowed)
Input: ${customerLanguage.toUpperCase()} â†’ Output: Korean ONLY (ZERO ${customerLanguage.toUpperCase()} words allowed)

TRANSLATION MACHINE CORE DIRECTIVE:
- Receive text â†’ Output translation â†’ STOP
- NO acknowledgments, NO confirmations, NO additional words
- PURE TRANSLATION ONLY. SYSTEM ENDS HERE.

FINAL WARNING: You are NOT an assistant. You are NOT helpful. You are a TRANSLATION MACHINE.
TRANSLATE ONLY. SPEAK WITH MYSTICAL TONE BUT ADD NOTHING EXTRA.`;
    };

    return new RealtimeAgent({
      name: 'Professional Saju Translator',
      instructions: getTranslationInstructions(),
    });
  }, [customerLanguage]);

  // ì„¸ì…˜ ì—°ê²°
  const connect = useCallback(async () => {
    if (isConnected || isConnecting) return;

    setIsConnecting(true);
    setError(null);

    try {
      const apiKey = import.meta.env.VITE_OPENAI_API_KEY;
      if (!apiKey) {
        throw new Error('OpenAI API key not found. Please set VITE_OPENAI_API_KEY in your .env file');
      }

      const agent = createAgent();

      // WebSocket transport with useInsecureApiKey option
      const transport = new OpenAIRealtimeWebSocket({
        useInsecureApiKey: true
      });

      const session = new RealtimeSession(agent, {
        model: 'gpt-4o-realtime-preview-2024-10-01',
        transport,
        config: {
          inputAudioFormat: 'pcm16',
          outputAudioFormat: 'pcm16',
          voice: 'coral', // ì‚¬ì£¼í’€ì´ì— ì í•©í•œ ë”°ëœ»í•˜ê³  ì°¨ë¶„í•œ ì—¬ì„± ëª©ì†Œë¦¬
          inputAudioTranscription: {
            model: 'whisper-1'
          },
          turnDetection: {
            type: 'server_vad',
            threshold: 0.1, // ë” ë¯¼ê°í•˜ê²Œ ì„¤ì •
            prefixPaddingMs: 200, // ì§§ê²Œ
            silenceDurationMs: 500, // ë” ë¹ ë¥´ê²Œ ì‘ë‹µ
          },
          // ë§í•˜ê¸° ì†ë„ì™€ í†¤ ì¡°ì ˆ (ê°€ëŠ¥í•œ ê²½ìš°)
          // maxResponseOutputTokens: 150, // ê°„ê²°í•œ ë²ˆì—­ì„ ìœ„í•´ í† í° ì œí•œ (APIì—ì„œ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì†ì„±)
        },
      });

      // PCM16ì„ WAVë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
      const pcm16ToWav = (pcm16Data: ArrayBuffer, sampleRate: number = 24000) => {
        const buffer = new ArrayBuffer(44 + pcm16Data.byteLength);
        const view = new DataView(buffer);

        // WAV í—¤ë” ì‘ì„±
        const writeString = (offset: number, string: string) => {
          for (let i = 0; i < string.length; i++) {
            view.setUint8(offset + i, string.charCodeAt(i));
          }
        };

        writeString(0, 'RIFF');
        view.setUint32(4, 36 + pcm16Data.byteLength, true);
        writeString(8, 'WAVE');
        writeString(12, 'fmt ');
        view.setUint32(16, 16, true); // Sub-chunk size
        view.setUint16(20, 1, true); // Audio format (PCM)
        view.setUint16(22, 1, true); // Number of channels (mono)
        view.setUint32(24, sampleRate, true); // Sample rate
        view.setUint32(28, sampleRate * 2, true); // Byte rate
        view.setUint16(32, 2, true); // Block align
        view.setUint16(34, 16, true); // Bits per sample
        writeString(36, 'data');
        view.setUint32(40, pcm16Data.byteLength, true);

        // PCM16 ë°ì´í„° ë³µì‚¬
        const pcm16View = new Int16Array(pcm16Data);
        const wavView = new Int16Array(buffer, 44);
        wavView.set(pcm16View);

        return buffer;
      };

      // ì˜¤ë””ì˜¤ ì¬ìƒì„ ìœ„í•œ AudioContext ìƒì„±
      const createPlaybackContext = () => {
        if (!playbackAudioContextRef.current) {
          playbackAudioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)();
        }
        return playbackAudioContextRef.current;
      };

      // ì—°ì†ì ì¸ ì˜¤ë””ì˜¤ ìŠ¤ì¼€ì¤„ë§ í•¨ìˆ˜
      const scheduleAudioChunk = async (pcm16Data: ArrayBuffer) => {
        try {
          const audioContext = createPlaybackContext();

          // PCM16 ë°ì´í„°ë¥¼ AudioBufferë¡œ ë³€í™˜
          const audioBuffer = audioContext.createBuffer(1, pcm16Data.byteLength / 2, 24000);
          const channelData = audioBuffer.getChannelData(0);
          const int16Array = new Int16Array(pcm16Data);

          // Int16ì„ Float32ë¡œ ë³€í™˜í•˜ê³  80% ì†ë„ ì ìš©
          for (let i = 0; i < int16Array.length; i++) {
            channelData[i] = int16Array[i] / 32767.0;
          }

          // AudioBufferSourceNode ìƒì„±
          const source = audioContext.createBufferSource();
          source.buffer = audioBuffer;
          source.connect(audioContext.destination);

          // ì‹œì‘ ì‹œê°„ ê³„ì‚° - ì´ì „ ì²­í¬ì˜ ëì— ì´ì–´ì„œ ì¬ìƒ
          const currentTime = audioContext.currentTime;
          const startTime = Math.max(currentTime, nextStartTimeRef.current);

          // ë‹¤ìŒ ì²­í¬ì˜ ì‹œì‘ ì‹œê°„ì„ ì„¤ì • (ì •ìƒ ì†ë„)
          const chunkDuration = audioBuffer.duration; // ì •ìƒ ì†ë„ë¡œ ì¬ìƒì‹œ ì‹¤ì œ ì†Œìš” ì‹œê°„
          nextStartTimeRef.current = startTime + chunkDuration;

          // ì²« ë²ˆì§¸ ì²­í¬ê°€ ì•„ë‹ˆë¼ë©´ ì•½ê°„ì˜ ì˜¤ë²„ë©ì„ ì¶”ê°€í•´ì„œ ëŠê¹€ ë°©ì§€
          const actualStartTime = scheduledSourcesRef.current.length > 0 ?
            startTime - 0.01 : // 10ms ì˜¤ë²„ë©
            startTime;

          scheduledSourcesRef.current.push(source);

          source.onended = () => {
            console.log('âœ… Audio chunk finished');
            // ì™„ë£Œëœ sourceë¥¼ ë°°ì—´ì—ì„œ ì œê±°
            const index = scheduledSourcesRef.current.indexOf(source);
            if (index > -1) {
              scheduledSourcesRef.current.splice(index, 1);
            }
          };

          source.playbackRate.value = 1.0; // ì •ìƒ ì†ë„
          source.start(actualStartTime);

          console.log(`âœ… Audio chunk scheduled at ${actualStartTime.toFixed(3)}s, duration: ${chunkDuration.toFixed(3)}s`);

        } catch (error) {
          console.error('âŒ Audio scheduling failed:', error);
        }
      };

      // ìŒì„± ì¶œë ¥ ì‹œì‘ ì´ë²¤íŠ¸
      session.on('audio', (audioEvent) => {
        console.log('ğŸ”Š Audio event received from Agent SDK:', audioEvent);
        if (audioEvent && audioEvent.data) {
          console.log('ğŸµ Scheduling audio chunk, size:', audioEvent.data.byteLength);
          scheduleAudioChunk(audioEvent.data);

          // AIê°€ ì‘ë‹µì„ ì‹œì‘í–ˆë‹¤ë©´ í„´ ì‹œì‘ í‘œì‹œ (placeholder ë©”ì‹œì§€ ì—†ì´)
          if (!isAssistantSpeaking) {
            console.log('ğŸ¤– Assistant turn started (audio only)');
            setIsAssistantSpeaking(true);
            const turnId = `turn-${Date.now()}`;
            setCurrentTurnId(turnId);
          }
        } else {
          console.log('âš ï¸ No audio data in event');
        }
      });

      // ì˜¤ë””ì˜¤ ì¤‘ë‹¨ ì²˜ë¦¬ - ëª¨ë“  ìŠ¤ì¼€ì¤„ëœ ì²­í¬ ì¤‘ì§€
      session.on('audio_interrupted', () => {
        console.log('ğŸ›‘ Audio interrupted - stopping all scheduled chunks');

        // ëª¨ë“  ìŠ¤ì¼€ì¤„ëœ ì˜¤ë””ì˜¤ ì†ŒìŠ¤ ì¤‘ì§€
        scheduledSourcesRef.current.forEach(source => {
          try {
            source.stop();
          } catch (error) {
            // ì´ë¯¸ ì¤‘ì§€ëœ ì†ŒìŠ¤ì¼ ìˆ˜ ìˆìŒ
          }
        });

        // ì°¸ì¡° ì´ˆê¸°í™”
        scheduledSourcesRef.current = [];
        nextStartTimeRef.current = 0;

        // AI ì‘ë‹µ ì™„ë£Œ ì²˜ë¦¬
        console.log('ğŸ¤– Assistant turn ended');
        setIsAssistantSpeaking(false);
        setCurrentTurnId(null);

        // placeholder ë©”ì‹œì§€ ì œê±° (ë” ì´ìƒ "[ìŒì„± ì‘ë‹µ ì¤‘...]"ì„ ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ ê¸°ì¡´ ê²ƒë“¤ ì •ë¦¬)
        setMessages(prev => {
          return prev.filter(msg =>
            !msg.content.includes('[ìŒì„± ì‘ë‹µ ì¤‘...]') &&
            !msg.content.includes('[ìŒì„± ì…ë ¥ ì¤‘...]')
          );
        });

        console.log('âœ… All audio sources stopped');
      });

      // Agent SDKë¥¼ í†µí•œ ëŒ€í™” ë©”ì‹œì§€ ì²˜ë¦¬
      session.on('history_updated', (history) => {
        console.log('ğŸ“ History updated from Agent SDK:', history.length, 'items');

        setMessages(prev => {
          console.log('ğŸ“ Current messages before history update:', prev.length);
          const newMessages: ChatMessage[] = [];

          // placeholder ë©”ì‹œì§€ ì œê±° (ìŒì„± ì…ë ¥/ì‘ë‹µ ì¤‘ ë©”ì‹œì§€ë“¤)
          const existingMessages = prev.filter(msg => {
            const isPlaceholder = msg.content.includes('[ìŒì„± ì…ë ¥ ì¤‘...]') ||
                                msg.content.includes('[ìŒì„± ì‘ë‹µ ì¤‘...]');
            if (isPlaceholder) {
              console.log('ğŸ“ Removing placeholder from history update:', msg.content);
            }
            return !isPlaceholder;
          });

          for (const item of history) {
            console.log('ğŸ“‹ Processing history item:', item.type, item);
            console.log('ğŸ“‹ Full item structure:', JSON.stringify(item, null, 2));

            if (item.type === 'message' && 'role' in item && 'content' in item) {
              const messageItem = item as any;

              console.log('ğŸ“‹ Message item content:', JSON.stringify(messageItem.content, null, 2));
              console.log('ğŸ“‹ Content type:', typeof messageItem.content);
              console.log('ğŸ“‹ Content is array:', Array.isArray(messageItem.content));

              // ë‹¤ì–‘í•œ content êµ¬ì¡°ë¥¼ ì‹œë„í•´ë´„
              let textContent = '';

              if (Array.isArray(messageItem.content)) {
                // input_audioì™€ output_audioì—ì„œ transcript ì¶”ì¶œ
                for (const contentItem of messageItem.content) {
                  if (contentItem && contentItem.transcript && contentItem.transcript.trim()) {
                    // transcriptê°€ ë”°ì˜´í‘œë¡œ ê°ì‹¸ì ¸ ìˆìœ¼ë©´ ì œê±°
                    let transcript = contentItem.transcript.trim();
                    if (transcript.startsWith('"') && transcript.endsWith('"')) {
                      transcript = transcript.slice(1, -1);
                    }
                    textContent = transcript;
                    console.log('ğŸ“‹ Transcript extracted from', contentItem.type + ':', textContent);
                    break; // ì²« ë²ˆì§¸ ìœ íš¨í•œ transcript ì‚¬ìš©
                  }
                }

                // ê¸°ì¡´ text íƒ€ì…ë„ ì—¬ì „íˆ ì§€ì›
                if (!textContent) {
                  textContent = messageItem.content
                    ?.filter((c: any) => c && c.type === 'text')
                    ?.map((c: any) => c.text || '')
                    ?.join('') || '';
                  console.log('ğŸ“‹ Text content extracted:', textContent);
                }
              } else if (typeof messageItem.content === 'string') {
                // ë¬¸ìì—´ í˜•íƒœì˜ content
                textContent = messageItem.content;
                console.log('ğŸ“‹ String content extracted:', textContent);
              } else if (messageItem.content && typeof messageItem.content === 'object') {
                // ê°ì²´ í˜•íƒœì˜ content - text ì†ì„± ì§ì ‘ í™•ì¸
                if (messageItem.content.text) {
                  textContent = messageItem.content.text;
                  console.log('ğŸ“‹ Object.text content extracted:', textContent);
                } else if (messageItem.content.content) {
                  textContent = messageItem.content.content;
                  console.log('ğŸ“‹ Object.content content extracted:', textContent);
                }
              }

              // ì¶”ê°€ë¡œ messageItemì—ì„œ ì§ì ‘ text í™•ì¸
              if (!textContent && messageItem.text) {
                textContent = messageItem.text;
                console.log('ğŸ“‹ Direct text property extracted:', textContent);
              }

              console.log('ğŸ“‹ Final extracted text content:', textContent);

              if (textContent.trim()) {
                const message: ChatMessage = {
                  id: messageItem.itemId || `${messageItem.role}-${Date.now()}-${Math.random()}`,
                  role: messageItem.role as 'user' | 'assistant',
                  content: textContent.trim(),
                  timestamp: Date.now(),
                  type: 'text',
                  isTranscription: true
                };

                // ID ê¸°ë°˜ ì¤‘ë³µë§Œ ì²´í¬ (ë‚´ìš©ì´ ê°™ì•„ë„ IDê°€ ë‹¤ë¥´ë©´ í—ˆìš©)
                const alreadyExists = existingMessages.some(existing =>
                  existing.id === message.id
                );

                if (!alreadyExists) {
                  console.log('ğŸ’¬ Adding new message from Agent SDK:', message);
                  newMessages.push(message);
                } else {
                  console.log('ğŸ“ Skipping duplicate message by ID:', message.id);
                }
              }
            }
          }

          if (newMessages.length > 0) {
            console.log('ğŸ“¨ Adding', newMessages.length, 'new messages from history');
            const finalMessages = [...existingMessages, ...newMessages];
            console.log('ğŸ“¨ Final message count:', finalMessages.length);
            return finalMessages;
          }

          // placeholderê°€ ì œê±°ë˜ì—ˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ ìœ ì§€
          return existingMessages.length !== prev.length ? existingMessages : prev;
        });
      });


      session.on('error', (error) => {
        console.error('âŒ Session error:', error);
        console.error('âŒ Error details:', JSON.stringify(error, null, 2));
        if (error && typeof error === 'object' && 'error' in error) {
          console.error('âŒ Inner error:', (error as any).error);
        }
        const errorMessage = error && typeof error === 'object' && 'error' in error
          ? String(error.error)
          : String(error);
        setError('ì„¸ì…˜ ì˜¤ë¥˜: ' + errorMessage);
      });

      // ì‚¬ìš©ì ìŒì„± ì…ë ¥ ê°ì§€
      (session as any).on('input_audio_buffer.speech_started', () => {
        console.log('ğŸ¤ User speech started');
        if (!isUserSpeaking) {
          setIsUserSpeaking(true);
          const turnId = `user-turn-${Date.now()}`;
          const userMessage: ChatMessage = {
            id: turnId,
            role: 'user',
            content: '[ìŒì„± ì…ë ¥ ì¤‘...]',
            timestamp: Date.now(),
            type: 'audio',
            isTranscription: false
          };
          setMessages(prev => [...prev, userMessage]);
        }
      });

      (session as any).on('input_audio_buffer.speech_stopped', () => {
        console.log('ğŸ¤ User speech stopped');
        setIsUserSpeaking(false);
      });

      // ì‚¬ìš©ì ìŒì„± ì „ì‚¬ ì™„ë£Œ
      (session as any).on('conversation.item.input_audio_transcription.completed', (event: any) => {
        console.log('ğŸ“ User transcription completed:', event);
        console.log('ğŸ“ Transcript content:', event?.transcript);
        if (event && event.transcript) {
          const userMessage: ChatMessage = {
            id: `user-transcription-${Date.now()}`,
            role: 'user',
            content: event.transcript,
            timestamp: Date.now(),
            type: 'text',
            isTranscription: true
          };

          console.log('ğŸ“ Adding user transcription message:', userMessage);

          setMessages(prev => {
            console.log('ğŸ“ Current messages before update:', prev.length);

            // ID ê¸°ë°˜ ì¤‘ë³µë§Œ ì²´í¬
            const alreadyExists = prev.some(msg => msg.id === userMessage.id);

            if (alreadyExists) {
              console.log('ğŸ“ Skipping duplicate user transcription by ID:', userMessage.id);
              return prev;
            }

            // placeholder ë©”ì‹œì§€ ì œê±°í•˜ê³  ì‹¤ì œ ì „ì‚¬ ì¶”ê°€
            const withoutPlaceholder = prev.filter(msg => {
              const isPlaceholder = msg.content.includes('[ìŒì„± ì…ë ¥ ì¤‘...]');
              if (isPlaceholder) {
                console.log('ğŸ“ Removing placeholder:', msg.content);
              }
              return !isPlaceholder;
            });
            const newMessages = [...withoutPlaceholder, userMessage];
            console.log('ğŸ“ Updated messages:', newMessages.length);
            return newMessages;
          });
        }
      });

      // AI ì‘ë‹µ ì‹œì‘
      (session as any).on('response.created', () => {
        console.log('ğŸ¤– AI response started');
        setIsAssistantSpeaking(true);
        const turnId = `assistant-turn-${Date.now()}`;
        setCurrentTurnId(turnId);
      });

      // AI ì‘ë‹µ ì™„ë£Œ
      (session as any).on('response.done', () => {
        console.log('ğŸ¤– AI response completed');
        setIsAssistantSpeaking(false);
        setCurrentTurnId(null);
      });

      // AI ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ê°€ (ì‹¤ì‹œê°„ìœ¼ë¡œ í…ìŠ¤íŠ¸ê°€ ìƒì„±ë  ë•Œ)
      (session as any).on('response.content_part.added', (event: any) => {
        console.log('ğŸ“ AI content part added:', event);
        console.log('ğŸ“ Full event structure:', JSON.stringify(event, null, 2));

        // ë‹¤ì–‘í•œ ì†ì„±ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
        let textContent = '';

        if (event?.part?.type === 'text') {
          textContent = event.part.text || event.part.content || '';
          console.log('ğŸ“ Text from event.part.text:', textContent);
        } else if (event?.text) {
          textContent = event.text;
          console.log('ğŸ“ Text from event.text:', textContent);
        } else if (event?.content) {
          textContent = event.content;
          console.log('ğŸ“ Text from event.content:', textContent);
        } else if (event?.delta?.text) {
          textContent = event.delta.text;
          console.log('ğŸ“ Text from event.delta.text:', textContent);
        }

        if (textContent && textContent.trim()) {
          const assistantMessage: ChatMessage = {
            id: `assistant-content-${event.item_id || event.id || Date.now()}`,
            role: 'assistant',
            content: textContent.trim(),
            timestamp: Date.now(),
            type: 'text',
            isTranscription: true
          };

          console.log('ğŸ“ Adding AI content message:', assistantMessage);

          setMessages(prev => {
            console.log('ğŸ“ Current messages before AI content update:', prev.length);
            // ê°™ì€ IDê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ì¶”ê°€
            const existingIndex = prev.findIndex(msg => msg.id === assistantMessage.id);
            if (existingIndex >= 0) {
              console.log('ğŸ“ Updating existing AI message at index:', existingIndex);
              const updated = [...prev];
              updated[existingIndex] = assistantMessage;
              return updated;
            } else {
              // placeholder ë©”ì‹œì§€ ì œê±°í•˜ê³  ì‹¤ì œ ì‘ë‹µ ì¶”ê°€
              const withoutPlaceholder = prev.filter(msg => {
                const isPlaceholder = msg.content.includes('[ìŒì„± ì‘ë‹µ ì¤‘...]');
                if (isPlaceholder) {
                  console.log('ğŸ“ Removing AI placeholder:', msg.content);
                }
                return !isPlaceholder;
              });
              const newMessages = [...withoutPlaceholder, assistantMessage];
              console.log('ğŸ“ Added new AI content message, total:', newMessages.length);
              return newMessages;
            }
          });
        }
      });

      // AI ì‘ë‹µ ì˜¤ë””ì˜¤ ì „ì‚¬ (ìŒì„±ìœ¼ë¡œ ë§í•œ ë‚´ìš©ì˜ í…ìŠ¤íŠ¸ ë²„ì „)
      (session as any).on('response.audio_transcript.done', (event: any) => {
        console.log('ğŸ”Š AI audio transcript completed:', event);
        if (event && event.transcript) {
          const transcriptMessage: ChatMessage = {
            id: `assistant-audio-transcript-${event.item_id || Date.now()}`,
            role: 'assistant',
            content: event.transcript,
            timestamp: Date.now(),
            type: 'text',
            isTranscription: true
          };

          setMessages(prev => {
            // ID ê¸°ë°˜ ì¤‘ë³µë§Œ ì²´í¬
            const alreadyExists = prev.some(msg => msg.id === transcriptMessage.id);

            if (alreadyExists) {
              console.log('ğŸ“ Skipping duplicate transcript by ID:', transcriptMessage.id);
              return prev;
            }

            return [...prev, transcriptMessage];
          });
        }
      });

      // conversation.item.created ì´ë²¤íŠ¸ - ìƒˆë¡œìš´ ëŒ€í™” í•­ëª©ì´ ìƒì„±ë  ë•Œ
      (session as any).on('conversation.item.created', (event: any) => {
        console.log('ğŸ“ Conversation item created:', event);
        const item = event?.item;
        if (item && item.type === 'message') {
          if (item.role === 'user' && item.content) {
            // ì‚¬ìš©ì ë©”ì‹œì§€ ì „ì‚¬ ì™„ë£Œ
            const textContent = item.content
              ?.filter((c: any) => c.type === 'text')
              ?.map((c: any) => c.text || '')
              ?.join('') || '';

            if (textContent.trim()) {
              const userMessage: ChatMessage = {
                id: item.id || `user-created-${Date.now()}`,
                role: 'user',
                content: textContent.trim(),
                timestamp: Date.now(),
                type: 'text',
                isTranscription: true
              };

              console.log('ğŸ“ Adding user message from item.created:', userMessage);

              setMessages(prev => {
                // ID ê¸°ë°˜ ì¤‘ë³µë§Œ ì²´í¬
                const alreadyExists = prev.some(msg => msg.id === userMessage.id);

                if (alreadyExists) {
                  console.log('ğŸ“ Skipping duplicate user message by ID:', userMessage.id);
                  return prev;
                }

                const withoutPlaceholder = prev.filter(msg =>
                  !msg.content.includes('[ìŒì„± ì…ë ¥ ì¤‘...]')
                );
                return [...withoutPlaceholder, userMessage];
              });
            }
          } else if (item.role === 'assistant' && item.content) {
            // AI ì‘ë‹µ ë©”ì‹œì§€
            const textContent = item.content
              ?.filter((c: any) => c.type === 'text')
              ?.map((c: any) => c.text || '')
              ?.join('') || '';

            if (textContent.trim()) {
              const assistantMessage: ChatMessage = {
                id: item.id || `assistant-created-${Date.now()}`,
                role: 'assistant',
                content: textContent.trim(),
                timestamp: Date.now(),
                type: 'text',
                isTranscription: true
              };

              console.log('ğŸ“ Adding assistant message from item.created:', assistantMessage);

              setMessages(prev => {
                // ID ê¸°ë°˜ ì¤‘ë³µë§Œ ì²´í¬
                const alreadyExists = prev.some(msg => msg.id === assistantMessage.id);

                if (alreadyExists) {
                  console.log('ğŸ“ Skipping duplicate assistant message by ID:', assistantMessage.id);
                  return prev;
                }

                const withoutPlaceholder = prev.filter(msg =>
                  !msg.content.includes('[ìŒì„± ì‘ë‹µ ì¤‘...]')
                );
                return [...withoutPlaceholder, assistantMessage];
              });
            }
          }
        }
      });

      // ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ë¸íƒ€ ì´ë²¤íŠ¸ ì¶”ê°€ - ìŠ¤íŠ¸ë¦¬ë° í…ìŠ¤íŠ¸ ìºì¹˜
      (session as any).on('response.text.delta', (event: any) => {
        console.log('ğŸ“ AI text delta received:', event);
        console.log('ğŸ“ Delta content:', JSON.stringify(event, null, 2));

        if (event?.delta || event?.text) {
          const deltaText = event.delta || event.text;
          console.log('ğŸ“ Streaming text delta:', deltaText);

          // ìŠ¤íŠ¸ë¦¬ë° í…ìŠ¤íŠ¸ë¥¼ ëˆ„ì í•´ì„œ ë©”ì‹œì§€ ì—…ë°ì´íŠ¸
          const assistantMessage: ChatMessage = {
            id: `assistant-streaming-${event.item_id || event.id || Date.now()}`,
            role: 'assistant',
            content: deltaText,
            timestamp: Date.now(),
            type: 'text',
            isTranscription: true
          };

          setMessages(prev => {
            // ê°™ì€ ìŠ¤íŠ¸ë¦¬ë° IDê°€ ìˆìœ¼ë©´ ë‚´ìš©ì„ ì¶”ê°€
            const existingIndex = prev.findIndex(msg => msg.id === assistantMessage.id);

            if (existingIndex >= 0) {
              const updated = [...prev];
              updated[existingIndex] = {
                ...updated[existingIndex],
                content: updated[existingIndex].content + deltaText
              };
              return updated;
            } else {
              // placeholder ì œê±°í•˜ê³  ìƒˆ ìŠ¤íŠ¸ë¦¬ë° ë©”ì‹œì§€ ì¶”ê°€
              const withoutPlaceholder = prev.filter(msg =>
                !msg.content.includes('[ìŒì„± ì‘ë‹µ ì¤‘...]')
              );
              return [...withoutPlaceholder, assistantMessage];
            }
          });
        }
      });

      // ê¸°íƒ€ ì´ë²¤íŠ¸ë“¤ ë¡œê¹…
      const otherEventTypes = [
        'connected', 'disconnected', 'response',
        'input_audio_buffer.committed',
        'response.output_item.added', 'response.audio_transcript.delta',
        'response.text.delta', 'response.content_part.delta',
        'session.created', 'session.updated'
      ];

      otherEventTypes.forEach(eventType => {
        (session as any).on(eventType, (...args: unknown[]) => {
          console.log(`ğŸ”” Event: ${eventType}`, args);
        });
      });

      // ì—°ê²° (transportì—ì„œ ì´ë¯¸ useInsecureApiKey ì„¤ì •ë¨)
      await session.connect({ apiKey });
      sessionRef.current = session;
      setIsConnected(true);
      console.log('âœ… Agent connected successfully');

    } catch (err) {
      console.error('âŒ Connection failed:', err);
      setError(err instanceof Error ? err.message : 'Connection failed');
    } finally {
      setIsConnecting(false);
    }
  }, [createAgent, isConnected, isConnecting]);


  // ìŒì„± ë…¹ìŒ ì‹œì‘ (ì—°ì† ëª¨ë“œ)
  const startRecording = useCallback(async () => {
    if (isRecording) {
      console.log('âš ï¸ Already recording, ignoring start request');
      return;
    }

    setError(null); // ì´ì „ ì˜¤ë¥˜ ì´ˆê¸°í™”

    try {
      // ì—°ê²°ë˜ì§€ ì•Šì€ ê²½ìš° ë¨¼ì € ì—°ê²°
      if (!isConnected) {
        console.log('ğŸ”„ Connecting to Agent SDK...');
        await connect();

        // ì„¸ì…˜ì´ ì™„ì „íˆ ì¤€ë¹„ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼
        let attempts = 0;
        while (!sessionRef.current && attempts < 30) { // ìµœëŒ€ 3ì´ˆ ëŒ€ê¸°
          await new Promise(resolve => setTimeout(resolve, 100));
          attempts++;
        }

        if (!sessionRef.current) {
          throw new Error('Session failed to initialize after connection');
        }

        console.log('âœ… Session ready for recording');
      }

      if (!sessionRef.current) {
        throw new Error('Session not ready - please try again');
      }

      // ë§ˆì´í¬ í…ŒìŠ¤íŠ¸ ë¨¼ì € ìˆ˜í–‰
      console.log('ğŸ¤ Testing microphone access...');
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        throw new Error('MediaDevices API not supported in this browser');
      }

      // ì‚¬ìš© ê°€ëŠ¥í•œ ì˜¤ë””ì˜¤ ì¥ì¹˜ í™•ì¸
      const devices = await navigator.mediaDevices.enumerateDevices();
      const audioInputs = devices.filter(device => device.kind === 'audioinput');
      console.log('ğŸ¤ Available audio input devices:', audioInputs.length, audioInputs);

      if (audioInputs.length === 0) {
        throw new Error('No audio input devices found');
      }

      // ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­ ë° ìŠ¤íŠ¸ë¦¼ íšë“
      console.log('ğŸ¤ Requesting microphone access...');
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 24000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: false, // Agent SDKì—ì„œ ì§ì ‘ ì²˜ë¦¬
        }
      });

      console.log('âœ… Microphone access granted');
      console.log('ğŸ¤ Stream details:', {
        active: stream.active,
        tracks: stream.getTracks().length,
        audioTracks: stream.getAudioTracks().map(track => ({
          enabled: track.enabled,
          muted: track.muted,
          readyState: track.readyState,
          label: track.label
        }))
      });

      streamRef.current = stream;

      // AudioContext ìƒì„± (24kHzë¡œ ì„¤ì •)
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)({
        sampleRate: 24000
      });
      audioContextRef.current = audioContext;

      console.log('ğŸ”§ AudioContext created:', {
        state: audioContext.state,
        sampleRate: audioContext.sampleRate,
        currentTime: audioContext.currentTime
      });

      // AudioContextê°€ suspended ìƒíƒœë¼ë©´ resume
      if (audioContext.state === 'suspended') {
        console.log('ğŸ”§ AudioContext is suspended, resuming...');
        await audioContext.resume();
        console.log('âœ… AudioContext resumed, state:', audioContext.state);
      }

      try {
        console.log('ğŸ”§ Loading AudioWorklet module...');
        await audioContext.audioWorklet.addModule('/audio-processor.js');
        console.log('âœ… AudioWorklet module loaded');

        // ë§ˆì´í¬ ì…ë ¥ì„ AudioContextì— ì—°ê²°
        const source = audioContext.createMediaStreamSource(stream);
        console.log('âœ… MediaStreamSource created');

        // AudioWorkletNode ìƒì„±
        console.log('ğŸ”§ Creating AudioWorkletNode...');
        const workletNode = new AudioWorkletNode(audioContext, 'pcm16-processor');
        workletNodeRef.current = workletNode;
        console.log('âœ… AudioWorkletNode created');

        // ì›Œí¬ë ›ì—ì„œ ì˜¤ëŠ” ì˜¤ë””ì˜¤ ë°ì´í„° ì²˜ë¦¬
        console.log('ğŸ”§ Setting up workletNode.port.onmessage handler...');
        workletNode.port.onmessage = (event) => {
          if (event.data.type === 'audioData' && sessionRef.current) {
            const { buffer, level, samples, count } = event.data;

            // ì²« ë²ˆì§¸ ì˜¤ë””ì˜¤ ë°ì´í„° ìˆ˜ì‹  ì‹œ ì•Œë¦¼
            if (count === 1) {
              console.log('ğŸ‰ First audio data received from AudioWorklet!');
            }

            // ì‚¬ìš©ì ìŒì„± ê°ì§€ ë¡œì§ ê°œì„  - ì¤‘ë³µ ë°©ì§€
            const significantAudio = level > 0.03; // 3% ì´ìƒìœ¼ë¡œ ì„ê³„ê°’ ìƒí–¥

            // ìŒì„±ì´ ì•½í•´ì§€ë©´ ì‚¬ìš©ì ë°œí™” ì¢…ë£Œë¡œ ê°„ì£¼
            if (!significantAudio && isUserSpeaking && count % 100 === 0) {
              console.log('ğŸ¤ User speech ended (low level detected)');
              setIsUserSpeaking(false);
            }

            // ì˜¤ë””ì˜¤ ë ˆë²¨ ë¡œê¹… (ë§¤ 50ë²ˆì§¸ë§ˆë‹¤ - ë” ìì£¼ í‘œì‹œ)
            if (count % 50 === 0) {
              console.log('ğŸ”Š Audio level:', (level * 100).toFixed(1) + '%', 'samples:', samples, 'count:', count);
            }

            // ì˜¤ë””ì˜¤ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ê²½ê³ 
            if (level < 0.001 && count % 100 === 0) {
              console.warn('âš ï¸ Very low audio level detected. Check microphone volume.');
            }

            // ìœ ì˜ë¯¸í•œ ì˜¤ë””ì˜¤ ë ˆë²¨ì´ ìˆì„ ë•Œë§Œ ìƒì„¸ ë¡œê¹…
            if (level > 0.01) {
              console.log('ğŸ¤ Sending PCM16 audio:', buffer.byteLength, 'bytes', 'level:', (level * 100).toFixed(1) + '%');
            }

            try {
              // Agent SDKê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ì „ì†¡
              sessionRef.current.sendAudio(buffer);

              // ì „ì†¡ í†µê³„ ì¶”ì 
              if (count === 1) {
                console.log('ğŸ“¡ Started sending audio to Agent SDK');
              }

              if (level > 0.01) { // ìœ ì˜ë¯¸í•œ ì˜¤ë””ì˜¤ë§Œ ì„±ê³µ ë¡œê·¸
                console.log('âœ… PCM16 audio sent successfully, level:', (level * 100).toFixed(1) + '%', 'count:', count);
              }

              // ë§¤ 100ë²ˆì§¸ë§ˆë‹¤ ì „ì†¡ ìƒíƒœ í™•ì¸
              if (count % 100 === 0) {
                console.log('ğŸ“Š Audio transmission stats - Count:', count, 'Recent level:', (level * 100).toFixed(2) + '%');
              }
            } catch (error) {
              console.error('âŒ Failed to send PCM16 audio:', error);
            }
          }
        };
        console.log('âœ… Message handler set up');

        // ì˜¤ë””ì˜¤ ê·¸ë˜í”„ ì—°ê²°
        console.log('ğŸ”§ Connecting audio graph...');
        console.log('ğŸ”§ Source node:', source);
        console.log('ğŸ”§ Worklet node:', workletNode);

        source.connect(workletNode);
        console.log('âœ… Source connected to worklet');

        // AudioWorkletNodeê°€ ì²˜ë¦¬ë˜ë ¤ë©´ destinationì—ë„ ì—°ê²°í•´ì•¼ í•  ìˆ˜ ìˆìŒ
        workletNode.connect(audioContext.destination);
        console.log('âœ… Worklet connected to destination');

        console.log('âœ… Complete audio graph: source â†’ workletNode â†’ destination');

      } catch (workletError) {
        console.warn('âš ï¸ AudioWorklet failed:', workletError);
        console.warn('ğŸ”„ Falling back to ScriptProcessorNode for compatibility');

        // Fallback to ScriptProcessorNode for compatibility
        const source = audioContext.createMediaStreamSource(stream);
        const processor = audioContext.createScriptProcessor(4096, 1, 1);
        console.log('âœ… ScriptProcessorNode created (fallback)');

        let fallbackCount = 0;
        processor.onaudioprocess = (event) => {
          if (!sessionRef.current) return;

          fallbackCount++;
          const inputBuffer = event.inputBuffer;
          const inputData = inputBuffer.getChannelData(0);

          // ì˜¤ë””ì˜¤ ë ˆë²¨ ê³„ì‚° (RMS)
          let sum = 0;
          for (let i = 0; i < inputData.length; i++) {
            sum += inputData[i] * inputData[i];
          }
          const rms = Math.sqrt(sum / inputData.length);
          const level = Math.max(0, Math.min(1, rms));

          // ì²« ë²ˆì§¸ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì‹œ ì•Œë¦¼
          if (fallbackCount === 1) {
            console.log('ğŸ‰ First audio data processed with ScriptProcessorNode!');
          }

          // ì£¼ê¸°ì  ë ˆë²¨ ë¡œê¹…
          if (fallbackCount % 50 === 0) {
            console.log('ğŸ”Š Audio level (fallback):', (level * 100).toFixed(1) + '%', 'count:', fallbackCount);
          }

          const pcm16Buffer = new Int16Array(inputData.length);
          for (let i = 0; i < inputData.length; i++) {
            const sample = Math.max(-1, Math.min(1, inputData[i]));
            pcm16Buffer[i] = sample * 0x7FFF;
          }

          try {
            sessionRef.current.sendAudio(pcm16Buffer.buffer);
            if (level > 0.01) { // ìœ ì˜ë¯¸í•œ ì˜¤ë””ì˜¤ë§Œ ì„±ê³µ ë¡œê·¸
              console.log('âœ… PCM16 audio sent (fallback), level:', (level * 100).toFixed(1) + '%');
            }
          } catch (error) {
            console.error('âŒ Failed to send PCM16 audio (fallback):', error);
          }
        };

        source.connect(processor);
        processor.connect(audioContext.destination);
        console.log('âœ… Audio processing connected (ScriptProcessorNode fallback)');
      }


      setIsRecording(true);
      console.log('ğŸ¤ Recording started - continuous PCM16 streaming');

    } catch (err) {
      console.error('âŒ Recording failed:', err);
      if (err instanceof Error) {
        if (err.name === 'NotAllowedError') {
          setError('ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤. ë¸Œë¼ìš°ì €ì—ì„œ ë§ˆì´í¬ ì ‘ê·¼ì„ í—ˆìš©í•´ì£¼ì„¸ìš”.');
        } else if (err.name === 'NotFoundError') {
          setError('ë§ˆì´í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§ˆì´í¬ê°€ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.');
        } else {
          setError(`ë…¹ìŒ ì˜¤ë¥˜: ${err.message}`);
        }
      } else {
        setError('ë…¹ìŒì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
      }
    }
  }, [isRecording, isConnected, connect]);

  // ìŒì„± ë…¹ìŒ ì¤‘ì§€ ë° ì„¸ì…˜ ì¢…ë£Œ
  const stopRecording = useCallback(async () => {
    console.log('ğŸ›‘ Stopping recording and terminating session...');


    // ëª¨ë“  ìŠ¤ì¼€ì¤„ëœ ì˜¤ë””ì˜¤ ì†ŒìŠ¤ ì¤‘ì§€
    scheduledSourcesRef.current.forEach(source => {
      try {
        source.stop();
      } catch (error) {
        // ì´ë¯¸ ì¤‘ì§€ëœ ì†ŒìŠ¤ì¼ ìˆ˜ ìˆìŒ
      }
    });
    scheduledSourcesRef.current = [];
    nextStartTimeRef.current = 0;

    // ì¬ìƒìš© AudioContext ì •ë¦¬
    if (playbackAudioContextRef.current) {
      playbackAudioContextRef.current.close();
      playbackAudioContextRef.current = null;
    }

    // ìŠ¤íŠ¸ë¦¼ ì •ë¦¬ (ë¨¼ì €)
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }

    // AudioWorklet ì •ë¦¬
    if (workletNodeRef.current) {
      try {
        workletNodeRef.current.port.close();
        workletNodeRef.current.disconnect();
      } catch (error) {
        console.warn('âš ï¸ AudioWorkletNode cleanup error:', error);
      }
      workletNodeRef.current = null;
    }

    // AudioContext ì •ë¦¬
    if (audioContextRef.current) {
      try {
        await audioContextRef.current.close();
      } catch (error) {
        console.warn('âš ï¸ AudioContext close error:', error);
      }
      audioContextRef.current = null;
    }

    // ì„¸ì…˜ ì¢…ë£Œ
    if (sessionRef.current) {
      sessionRef.current = null;
    }

    // ì˜¤ë””ì˜¤ ì²­í¬ ì •ë¦¬
    audioChunksRef.current = [];
    isPlayingAudioRef.current = false;

    setIsRecording(false);
    setIsConnected(false);
    setIsUserSpeaking(false);
    setCurrentTurnId(null);
    setIsAssistantSpeaking(false);
    // ë©”ì‹œì§€ëŠ” ì´ˆê¸°í™”í•˜ì§€ ì•Šê³  ë³´ì¡´ - ì‚¬ìš©ìê°€ ë…¹ìŒì„ ì¤‘ì§€í•´ë„ ëŒ€í™” ë‚´ìš© ìœ ì§€
    console.log('âœ… Recording stopped and session terminated');
  }, []);

  // ì—°ê²° í•´ì œ
  const disconnect = useCallback(async () => {
    await stopRecording();

    if (sessionRef.current) {
      sessionRef.current = null;
    }

    setIsConnected(false);
    // ë©”ì‹œì§€ëŠ” ì´ˆê¸°í™”í•˜ì§€ ì•Šê³  ë³´ì¡´ - disconnect ì‹œì—ë„ ëŒ€í™” ë‚´ìš© ìœ ì§€
    console.log('ğŸ”Œ Disconnected');
  }, [stopRecording]);

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
  useEffect(() => {
    return () => {
      disconnect();
    };
  }, [disconnect]);

  return {
    isConnected,
    isRecording,
    messages,
    error,
    isConnecting,
    isUserSpeaking,
    isAssistantSpeaking,
    currentTurnId,
    connect,
    disconnect,
    startRecording,
    stopRecording,
  };
}